{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.034433600Z",
     "start_time": "2024-07-14T14:33:50.012255700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import random\n",
    "from itertools import product\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FUNCTIONS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a339c1d26959b665"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "\n",
    "def generate_initial_combinations(features, num_combinations=50):\n",
    "    \"\"\"\n",
    "    Generates a list of initial combinations of features and percentiles.\n",
    "\n",
    "    This function takes a list of features and generates combinations with percentiles ranging\n",
    "    from 10 to 85 in steps of 5. It then shuffles the combinations and returns a specified number\n",
    "    of them.\n",
    "\n",
    "    Parameters:\n",
    "    features (list): A list of features to be combined with percentiles.\n",
    "    num_combinations (int): The number of combinations to return. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each containing a feature and a percentile.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the range and step of percentiles\n",
    "    percentiles = list(range(10, 90, 5))  # Percentiles from 10 to 85 inclusive\n",
    "    \n",
    "    # Generate all possible combinations of features and percentiles\n",
    "    all_combinations = list(product(features, percentiles))\n",
    "    \n",
    "    # Shuffle the list of all combinations to ensure randomness\n",
    "    random.shuffle(all_combinations)\n",
    "    \n",
    "    # Select the required number of combinations and format them into a list of dictionaries\n",
    "    return [{'feature': combo[0], 'percentile': combo[1]} for combo in all_combinations[:num_combinations]]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.299349600Z",
     "start_time": "2024-07-14T14:33:50.287809900Z"
    }
   },
   "id": "52fc5a7dcdb9f654",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def test_significance(metrics, group1, group2):\n",
    "    \"\"\"\n",
    "    Helper function to test significance between two groups for given metrics.\n",
    "\n",
    "    This function takes a list of metrics and two groups of data, performs the Mann-Whitney U test\n",
    "    to determine if there are significant differences between the groups for each metric, and\n",
    "    returns the p-values and significant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    metrics (list): A list of metric names to be tested for significance.\n",
    "    group1 (DataFrame): A pandas DataFrame containing the first group of data.\n",
    "    group2 (DataFrame): A pandas DataFrame containing the second group of data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists:\n",
    "        - p_values (list): A list of p-values for metrics that showed significant differences.\n",
    "        - significant_metrics (list): A list of metrics that showed significant differences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to store p-values and significant metrics\n",
    "    p_values = []\n",
    "    significant_metrics = []\n",
    "\n",
    "    # Loop through each metric to perform the significance test\n",
    "    for metric in metrics:\n",
    "        # Ensure that both groups have data for the current metric\n",
    "        if not group1[metric].empty and not group2[metric].empty:\n",
    "            # Perform the Mann-Whitney U test\n",
    "            u_stat, p_value = stats.mannwhitneyu(group1[metric], group2[metric], alternative='two-sided')\n",
    "            # Check if the p-value indicates a significant difference\n",
    "            if p_value < 0.05:\n",
    "                # Append the p-value and the metric to the respective lists\n",
    "                p_values.append(p_value)\n",
    "                significant_metrics.append(metric)\n",
    "    \n",
    "    # Return the lists of p-values and significant metrics\n",
    "    return p_values, significant_metrics\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.478668600Z",
     "start_time": "2024-07-14T14:33:50.448696900Z"
    }
   },
   "id": "e21a2e8e87e4a649",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def initial_iteration(initializations, feature_scales, metrics_columns):\n",
    "    \"\"\"\n",
    "    Performs the initial iteration to find significant features and percentiles.\n",
    "\n",
    "    This function takes a list of initializations, feature scales, and metric columns, \n",
    "    and performs statistical tests to find significant differences between groups \n",
    "    based on specified percentiles. It returns a sorted list of results containing \n",
    "    significant features and their associated metrics.\n",
    "\n",
    "    Parameters:\n",
    "    initializations (list): A list of dictionaries, each containing 'feature' and 'percentile' keys.\n",
    "    feature_scales (DataFrame): A pandas DataFrame containing the scaled features.\n",
    "    metrics_columns (list): A list of metric column names to be tested.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples, each containing a dictionary with initial feature details \n",
    "          and significant metrics, and a dictionary with 'A' and 'B' group values.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Iterate over each initialization\n",
    "    for init in initializations:\n",
    "        feature = init['feature']\n",
    "        percentile_cutoff = init['percentile']\n",
    "        p_values, significant_metrics = [], []\n",
    "\n",
    "        # Calculate the percentile value for the feature\n",
    "        percentile = np.percentile(feature_scales[feature], percentile_cutoff)\n",
    "        \n",
    "        # Split the data into above and below percentile groups\n",
    "        above_values = feature_scales[feature_scales[feature] > percentile]\n",
    "        below_values = feature_scales[feature_scales[feature] <= percentile]\n",
    "\n",
    "        # Perform the significance test for each metric\n",
    "        for metric in metrics_columns:\n",
    "            u_stat, p_value = stats.mannwhitneyu(below_values[metric], above_values[metric], alternative='two-sided')\n",
    "            if p_value < 0.05:\n",
    "                p_values.append(p_value)\n",
    "                significant_metrics.append(metric)\n",
    "\n",
    "        # If there are significant metrics, store the results\n",
    "        if p_values:\n",
    "            mean_p_value = np.mean(p_values)\n",
    "            results.append(({\n",
    "                'initial_feature': feature,\n",
    "                'initial_percentile': percentile_cutoff,\n",
    "                'significant_counts': len(p_values),\n",
    "                'mean_p_value': mean_p_value,\n",
    "                'lineage': [[{'feature': feature, 'percentile': percentile_cutoff, 'significant_metrics': significant_metrics}]]\n",
    "            },{\n",
    "                'A': above_values,\n",
    "                'B': below_values\n",
    "            }))\n",
    "            \n",
    "            # Sort the results based on the number of significant metrics and mean p-value\n",
    "            results.sort(key=lambda x: (-x[0]['significant_counts'], x[0]['mean_p_value']))\n",
    "\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.635223Z",
     "start_time": "2024-07-14T14:33:50.616175400Z"
    }
   },
   "id": "f48101acc8b321f",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def perform_iterations(previous_results, feature_scales, metrics_columns, features, add_count=False, num_combinations=100, min_samples=5, min_split=15, num_iterations=1):\n",
    "    \"\"\"\n",
    "    Perform iterative feature analysis to find significant features and percentiles.\n",
    "\n",
    "    This function iteratively analyzes feature combinations to identify significant differences\n",
    "    between groups based on specified percentiles. It extends previous results with new feature\n",
    "    combinations, performs statistical tests, and updates the results with significant metrics.\n",
    "\n",
    "    Parameters:\n",
    "    previous_results (list): A list of previous results containing initial feature details and clusters.\n",
    "    feature_scales (DataFrame): A pandas DataFrame containing the scaled features.\n",
    "    metrics_columns (list): A list of metric column names to be tested.\n",
    "    features (list): A list of feature names to be used in generating new combinations.\n",
    "    add_count (bool): Whether to add the count of significant metrics from previous iterations. Default is False.\n",
    "    num_combinations (int): The number of new combinations to generate. Default is 100.\n",
    "    min_samples (int): The minimum number of samples required for a valid split. Default is 5.\n",
    "    min_split (int): The minimum number of samples required to consider a cluster for splitting. Default is 15.\n",
    "    num_iterations (int): The number of iterations to perform. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples, each containing a dictionary with feature details and significant metrics, and a dictionary with clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base case: If no iterations are left, return the previous results\n",
    "    if num_iterations == 0:\n",
    "        return previous_results\n",
    "\n",
    "    extended_results = []\n",
    "\n",
    "    # Iterate over each result and its associated clusters\n",
    "    for result, clusters in previous_results:\n",
    "        # Generate new feature combinations, excluding the initial feature\n",
    "        for combo in generate_initial_combinations([f for f in features if f != result['initial_feature']], num_combinations):\n",
    "            new_feature = combo['feature']\n",
    "            new_percentile = combo['percentile']\n",
    "            lineage = result['lineage'].copy()\n",
    "            new_step = {'feature': new_feature, 'percentile': new_percentile, 'significant_metrics': []}\n",
    "            lineage.append([new_step])\n",
    "            new_value = np.percentile(feature_scales[new_feature], new_percentile)\n",
    "\n",
    "            final_clusters = {}\n",
    "            p_values = []\n",
    "            significant_metrics = []\n",
    "\n",
    "            # Iterate over each cluster\n",
    "            for key, data in clusters.items():\n",
    "                # If the cluster size is too small, keep it as is\n",
    "                if len(data) <= min_split:\n",
    "                    final_clusters[key] = data\n",
    "                    continue\n",
    "\n",
    "                # Split the data into above and below the new percentile\n",
    "                new_above = data[data[new_feature] > new_value]\n",
    "                new_below = data[data[new_feature] <= new_value]\n",
    "\n",
    "                # If either split is too small, keep the original cluster\n",
    "                if len(new_above) < min_samples or len(new_below) < min_samples:\n",
    "                    final_clusters[key] = data\n",
    "                    continue\n",
    "\n",
    "                # Test for significant differences between the new clusters\n",
    "                new_p_values, new_significant_metrics = test_significance(metrics_columns, new_above, new_below)\n",
    "                if new_p_values:\n",
    "                    p_values.extend(new_p_values)\n",
    "                    significant_metrics.extend(new_significant_metrics)\n",
    "                    final_clusters[key + 'A'] = new_above\n",
    "                    final_clusters[key + 'B'] = new_below\n",
    "                else:\n",
    "                    final_clusters[key] = data\n",
    "\n",
    "            new_step['significant_metrics'] = significant_metrics\n",
    "\n",
    "            # Calculate the count of significant metrics\n",
    "            if add_count:\n",
    "                count = len(p_values) + result[\"significant_counts\"]\n",
    "            else:\n",
    "                count = len(p_values)\n",
    "\n",
    "            # If there are significant metrics, store the results\n",
    "            if p_values:\n",
    "                mean_p_value = np.mean(p_values)\n",
    "                extended_results.append(({\n",
    "                    'initial_feature': result['initial_feature'],\n",
    "                    'initial_percentile': result['initial_percentile'],\n",
    "                    'new_feature': new_feature,\n",
    "                    'new_percentile': new_percentile,\n",
    "                    'significant_counts': count,\n",
    "                    'mean_p_value': mean_p_value,\n",
    "                    'lineage': lineage\n",
    "                }, final_clusters))\n",
    "\n",
    "        # Sort the results by significant counts and mean p-value\n",
    "        extended_results.sort(key=lambda x: (-x[0]['significant_counts'], x[0]['mean_p_value']))\n",
    "\n",
    "    # Recursively perform iterations with the updated results\n",
    "    return perform_iterations(extended_results[:10], feature_scales, metrics_columns, features, add_count, num_combinations, min_samples, min_split, num_iterations - 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.806295300Z",
     "start_time": "2024-07-14T14:33:50.782225800Z"
    }
   },
   "id": "799f7ed2445b7b56",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_top_configurations(configurations, feature_scales, metrics_columns, top_n=5):\n",
    "    \"\"\"\n",
    "    Analyze and print details of the top N configurations.\n",
    "\n",
    "    This function takes a list of configurations, feature scales, and metric columns, \n",
    "    and prints details for the top N configurations based on their significant features \n",
    "    and percentiles. It shows cluster sizes and mean values of metrics across clusters.\n",
    "\n",
    "    Parameters:\n",
    "    configurations (list): A list of configurations containing feature details and clusters.\n",
    "    feature_scales (DataFrame): A pandas DataFrame containing the scaled features.\n",
    "    metrics_columns (list): A list of metric column names to be analyzed.\n",
    "    top_n (int): The number of top configurations to analyze. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over the top N configurations\n",
    "    for init in configurations[:top_n]:\n",
    "        initial_feature = init[0]['initial_feature']\n",
    "        initial_percentile = init[0]['initial_percentile']\n",
    "        new_feature = init[0]['new_feature']\n",
    "        new_percentile = init[0]['new_percentile']\n",
    "\n",
    "        # Print the configuration details\n",
    "        print(f\"\\nConfiguration: Initial Feature={initial_feature}, Percentile={initial_percentile}, \"\n",
    "              f\"New Feature={new_feature}, New Percentile={new_percentile}\")\n",
    "\n",
    "        clusters = init[1]\n",
    "\n",
    "        # Calculate and print the sizes of each cluster\n",
    "        cluster_sizes = {key: len(feature_scales.loc[clusters[key].index]) for key in clusters}\n",
    "        print(\"Cluster Sizes:\")\n",
    "        for key, size in cluster_sizes.items():\n",
    "            print(f\"  {key}: {size}\")\n",
    "\n",
    "        # Calculate and print the mean values of metrics across clusters\n",
    "        metric_means = {metric: {key: feature_scales.loc[clusters[key].index][metric].mean() for key in clusters if\n",
    "                                 not feature_scales.loc[clusters[key].index][metric].empty} for metric in metrics_columns}\n",
    "\n",
    "        print(\"Metric Means Across Clusters:\")\n",
    "        for metric, means in metric_means.items():\n",
    "            mean_values = \", \".join(f\"{key}: {means[key]:.4f}\" for key in sorted(means))\n",
    "            print(f\"  {metric}: {mean_values}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:50.997683900Z",
     "start_time": "2024-07-14T14:33:50.971544500Z"
    }
   },
   "id": "c4490b6db990831a",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "# APPLICATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12fc0c5bb7da73bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#We read the predictive processing data and the scales, without having filled the NaN values\n",
    "predictive_processing_dataset = pd.read_excel(\"All_Features_dataset.xlsx\")\n",
    "scales = pd.read_excel(\"scales.xlsx\")\n",
    "scales.drop([\"SUBJECT_CODE\",\"Age\"],axis=1,inplace=True)\n",
    "\n",
    "#We join both datasets\n",
    "common_values = scales['EPRIME_CODE'].unique()\n",
    "predictive_processing_dataset = predictive_processing_dataset[predictive_processing_dataset['Subject'].isin(common_values)]\n",
    "feature_scales = pd.merge(left = predictive_processing_dataset,right=scales, left_on=\"Subject\",right_on=\"EPRIME_CODE\").drop([\"EPRIME_CODE\",\"Subject\"],axis=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:52.322285Z",
     "start_time": "2024-07-14T14:33:52.084278800Z"
    }
   },
   "id": "9b5eb30b58765b23",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do not use the metrics with many NaN values, as it would not be able to perform the statistical test. In order to use these metrics, imputation of scales has to be done"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "754ac32043d6b421"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "metrics_columns = ['PA',\n",
    " 'NA.',\n",
    " 'ERQ_CR',\n",
    " 'ERQ_ES',\n",
    " 'UPPSP_NU',\n",
    " 'UPPSP_PU',\n",
    " 'UPPSP_SS',\n",
    " 'UPPSP_PMD',\n",
    " 'UPPSP_PSV',\n",
    " 'BIS',\n",
    " 'BAS_RR',\n",
    " 'BAS_D',\n",
    " 'BAS_FS',\n",
    " 'TEPS_AF',\n",
    " 'TEPS_CF',\n",
    " 'SHS',\n",
    " 'FS',\n",
    " 'LOT_R',\n",
    " 'RRQ_Rum',\n",
    " 'RRQ_Ref',\n",
    " 'ASI_P',\n",
    " 'ASI_C',\n",
    " 'ASI_S',\n",
    " 'ASI_T'             \n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:52.781850300Z",
     "start_time": "2024-07-14T14:33:52.746294200Z"
    }
   },
   "id": "48b76087174829c3",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "The predictive processing features used. The list can be modified as pleased."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeb764e6669396a0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "specific_features = [\"Mean_Rating0\",\"Mean_Rating0_Match\",\"Mean_Rating0_No_Match\",\"Dif_Match\",\"Cor_Pred_Like\",\"Cor_Pred_Like_Match\",\"Cor_Pred_Like_No_Match\",\"Mean_Rating0_Match_Negative\",\"Mean_Rating0_No_Match_Negative\",\"Dif_Negative\",\"Trend_Match\",\"Trend_No_Match\",\n",
    "    \"Trend_No_Match_Negative\", \"Trend_Match_Negative\",\n",
    "    \"Cor_Pred_Like_Match_Negative\", \"Cor_Pred_Like_No_Match_Negative\",\"Mean_Rating0_Match_Happy\",\"Mean_Rating0_No_Match_Happy\",\"Dif_Happy\",\n",
    "    \"Trend_No_Match_Happy\", \"Trend_Match_Happy\",\n",
    "    \"Cor_Pred_Like_Match_Happy\", \"Cor_Pred_Like_No_Match_Happy\",\"Cor_Pred_Like_Negative\",\"Mean_Rating0_Negative\",\"Cor_Pred_Like_Happy\",\"Mean_Rating0_Happy\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:33:53.171937200Z",
     "start_time": "2024-07-14T14:33:53.157316Z"
    }
   },
   "id": "4c62c2f331edeaf2",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0f8a3977d68523b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "  - Feature: Mean_Rating0_No_Match_Happy at Percentile: 10\n",
      "Significant Counts: 26, Mean P-Value: 0.0142\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'significant_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  - Feature: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at Percentile: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpercentile\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSignificant Counts: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msignificant_counts\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Mean P-Value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_p_value\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSignificant Metrics: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msignificant_metrics\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'significant_metrics'"
     ]
    }
   ],
   "source": [
    "np.random.seed(99)  # For reproducibility\n",
    "features = specific_features \n",
    "initial_combinations = generate_initial_combinations(features, 50)\n",
    "initial_results = initial_iteration(initial_combinations, feature_scales[specific_features+metrics_columns].dropna().copy(), metrics_columns)\n",
    "final_results = perform_iterations(initial_results[:20], feature_scales[specific_features+metrics_columns].dropna().copy(), metrics_columns, features, add_count=True, min_samples=8, num_iterations=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:35:54.575029800Z",
     "start_time": "2024-07-14T14:33:53.874505100Z"
    }
   },
   "id": "429e60504b1eb3ca",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Mean_Rating0_No_Match_Happy at Percentile: 10\n",
      "    Significant Metrics: ['ERQ_ES', 'UPPSP_PSV', 'FS', 'LOT_R', 'ASI_C', 'ASI_S', 'ASI_T', 'UPPSP_PMD', 'BAS_RR', 'BAS_FS', 'LOT_R']\n",
      "Significant Counts: 26, Mean P-Value: 0.0142\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Mean_Rating0_No_Match_Happy at Percentile: 40\n",
      "    Significant Metrics: ['ERQ_ES', 'UPPSP_PSV', 'FS', 'LOT_R', 'ASI_C', 'ASI_S', 'ASI_T', 'UPPSP_PMD', 'BAS_RR', 'BAS_FS', 'LOT_R']\n",
      "Significant Counts: 26, Mean P-Value: 0.0142\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Mean_Rating0_No_Match_Happy at Percentile: 35\n",
      "    Significant Metrics: ['ERQ_ES', 'UPPSP_PSV', 'FS', 'LOT_R', 'ASI_C', 'ASI_S', 'ASI_T', 'UPPSP_PMD', 'BAS_RR', 'BAS_FS', 'LOT_R']\n",
      "Significant Counts: 26, Mean P-Value: 0.0142\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Trend_Match at Percentile: 40\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'BAS_RR', 'TEPS_AF', 'UPPSP_PMD', 'ASI_S', 'ASI_T', 'UPPSP_NU', 'UPPSP_PU', 'BAS_FS', 'TEPS_AF']\n",
      "Significant Counts: 26, Mean P-Value: 0.0253\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Trend_Match_Happy at Percentile: 35\n",
      "    Significant Metrics: ['UPPSP_SS', 'NA.', 'BIS', 'TEPS_AF', 'TEPS_CF', 'ASI_P', 'ASI_T', 'BIS', 'RRQ_Rum', 'ASI_C', 'ASI_T']\n",
      "Significant Counts: 26, Mean P-Value: 0.0281\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Cor_Pred_Like_Match_Happy at Percentile: 70\n",
      "    Significant Metrics: ['BIS', 'BAS_RR']\n",
      "  - Feature: Dif_Happy at Percentile: 80\n",
      "    Significant Metrics: ['PA', 'TEPS_CF', 'PA', 'ERQ_CR', 'UPPSP_PU', 'UPPSP_PMD', 'UPPSP_PSV', 'BAS_RR', 'FS', 'LOT_R', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Trend_Match_Negative at Percentile: 35\n",
      "    Significant Metrics: ['NA.', 'UPPSP_SS', 'UPPSP_PMD', 'BAS_FS', 'RRQ_Rum', 'UPPSP_PU', 'UPPSP_SS', 'BAS_D', 'BAS_FS']\n",
      "Significant Counts: 25, Mean P-Value: 0.0221\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Mean_Rating0_No_Match_Happy at Percentile: 20\n",
      "    Significant Metrics: ['ERQ_ES', 'BAS_RR', 'BAS_D', 'FS', 'LOT_R']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'TEPS_CF', 'FS', 'LOT_R', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Trend_Match_Negative at Percentile: 35\n",
      "    Significant Metrics: ['PA', 'UPPSP_SS', 'BAS_D', 'SHS', 'BIS', 'BAS_RR', 'NA.', 'UPPSP_SS', 'BAS_FS', 'ASI_S', 'ASI_T']\n",
      "Significant Counts: 24, Mean P-Value: 0.0186\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Cor_Pred_Like_No_Match_Negative at Percentile: 40\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'PA', 'NA.', 'ERQ_ES', 'UPPSP_SS', 'LOT_R', 'RRQ_Rum', 'ASI_T']\n",
      "Significant Counts: 24, Mean P-Value: 0.0243\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Trend_Match_Negative at Percentile: 35\n",
      "    Significant Metrics: ['PA', 'NA.', 'ERQ_ES', 'UPPSP_SS', 'BAS_D', 'ASI_T', 'UPPSP_SS', 'BAS_FS']\n",
      "Significant Counts: 23, Mean P-Value: 0.0188\n",
      "Feature Pathway for Cluster:\n",
      "  - Feature: Dif_Match at Percentile: 30\n",
      "    Significant Metrics: ['UPPSP_NU', 'UPPSP_PMD', 'UPPSP_PSV']\n",
      "  - Feature: Dif_Happy at Percentile: 70\n",
      "    Significant Metrics: ['PA', 'ERQ_CR', 'UPPSP_NU', 'UPPSP_PU', 'TEPS_CF', 'FS', 'LOT_R', 'RRQ_Rum', 'ASI_P', 'ASI_C', 'ASI_S', 'ASI_T']\n",
      "  - Feature: Cor_Pred_Like_Match at Percentile: 80\n",
      "    Significant Metrics: ['UPPSP_SS', 'BIS', 'ASI_S', 'FS', 'RRQ_Rum', 'ASI_P', 'ASI_S', 'ASI_T']\n",
      "Significant Counts: 23, Mean P-Value: 0.0195\n"
     ]
    }
   ],
   "source": [
    "for res, clusters in final_results:\n",
    "    print(f\"Feature Pathway for Cluster:\")\n",
    "    for step in res['lineage']:\n",
    "        print(f\"  - Feature: {step[0]['feature']} at Percentile: {step[0]['percentile']}\")\n",
    "        print(f\"    Significant Metrics: {step[0]['significant_metrics']}\")\n",
    "    print(f\"Significant Counts: {res['significant_counts']}, Mean P-Value: {res['mean_p_value']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T14:48:16.264421600Z",
     "start_time": "2024-07-14T14:48:16.233899Z"
    }
   },
   "id": "56f379a794a992cf",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualizing best results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cda2702e1d456e50"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration: Initial Feature=Dif_Match, Percentile=30, New Feature=Mean_Rating0_No_Match_Happy, New Percentile=10\n",
      "Cluster Sizes:\n",
      "  AAA: 11\n",
      "  AAB: 26\n",
      "  AB: 64\n",
      "  BA: 29\n",
      "  BB: 14\n",
      "Metric Means Across Clusters:\n",
      "  PA: AAA: 32.3636, AAB: 36.8846, AB: 31.9531, BA: 32.1724, BB: 34.2857\n",
      "  NA.: AAA: 20.5455, AAB: 18.2692, AB: 21.1250, BA: 20.0000, BB: 20.0000\n",
      "  ERQ_CR: AAA: 5.0609, AAB: 5.2176, AB: 4.7786, BA: 4.6728, BB: 5.0357\n",
      "  ERQ_ES: AAA: 4.0000, AAB: 2.5577, AB: 3.1094, BA: 3.1034, BB: 2.7679\n",
      "  UPPSP_NU: AAA: 7.6364, AAB: 7.6923, AB: 9.0625, BA: 9.3103, BB: 10.6429\n",
      "  UPPSP_PU: AAA: 9.4545, AAB: 9.0385, AB: 10.0938, BA: 9.8276, BB: 10.7857\n",
      "  UPPSP_SS: AAA: 10.4545, AAB: 11.7308, AB: 10.4688, BA: 11.4483, BB: 10.5714\n",
      "  UPPSP_PMD: AAA: 7.2727, AAB: 7.0769, AB: 7.1562, BA: 7.4138, BB: 9.2143\n",
      "  UPPSP_PSV: AAA: 7.9091, AAB: 5.9615, AB: 6.7969, BA: 7.5517, BB: 8.7143\n",
      "  BIS: AAA: 20.0000, AAB: 21.6538, AB: 22.3750, BA: 20.4138, BB: 21.9286\n",
      "  BAS_RR: AAA: 16.8182, AAB: 17.8077, AB: 17.6406, BA: 16.6207, BB: 18.0000\n",
      "  BAS_D: AAA: 10.9091, AAB: 12.2692, AB: 11.9375, BA: 11.3793, BB: 12.6429\n",
      "  BAS_FS: AAA: 11.9091, AAB: 12.4231, AB: 12.3750, BA: 12.5517, BB: 13.8571\n",
      "  TEPS_AF: AAA: 40.5455, AAB: 45.1538, AB: 42.6250, BA: 44.2069, BB: 43.5714\n",
      "  TEPS_CF: AAA: 34.5455, AAB: 38.7308, AB: 34.1094, BA: 36.4138, BB: 34.3571\n",
      "  SHS: AAA: 4.2273, AAB: 4.4327, AB: 4.1758, BA: 4.2500, BB: 4.4286\n",
      "  FS: AAA: 45.2727, AAB: 50.4231, AB: 46.4844, BA: 47.3448, BB: 48.1429\n",
      "  LOT_R: AAA: 14.1818, AAB: 17.2692, AB: 14.1875, BA: 13.8621, BB: 16.2143\n",
      "  RRQ_Rum: AAA: 3.4091, AAB: 2.9260, AB: 3.5419, BA: 3.1099, BB: 3.3521\n",
      "  RRQ_Ref: AAA: 3.7130, AAB: 3.2046, AB: 3.2089, BA: 3.3848, BB: 3.7740\n",
      "  ASI_P: AAA: 4.4545, AAB: 2.5000, AB: 5.6719, BA: 4.8621, BB: 4.5000\n",
      "  ASI_C: AAA: 3.8182, AAB: 1.4615, AB: 3.9688, BA: 3.5172, BB: 5.7857\n",
      "  ASI_S: AAA: 10.2727, AAB: 5.6923, AB: 9.2344, BA: 7.3793, BB: 8.5000\n",
      "  ASI_T: AAA: 18.5455, AAB: 9.6538, AB: 18.8750, BA: 15.7586, BB: 18.7857\n",
      "\n",
      "Configuration: Initial Feature=Dif_Match, Percentile=30, New Feature=Mean_Rating0_No_Match_Happy, New Percentile=40\n",
      "Cluster Sizes:\n",
      "  AAA: 11\n",
      "  AAB: 26\n",
      "  AB: 64\n",
      "  BA: 29\n",
      "  BB: 14\n",
      "Metric Means Across Clusters:\n",
      "  PA: AAA: 32.3636, AAB: 36.8846, AB: 31.9531, BA: 32.1724, BB: 34.2857\n",
      "  NA.: AAA: 20.5455, AAB: 18.2692, AB: 21.1250, BA: 20.0000, BB: 20.0000\n",
      "  ERQ_CR: AAA: 5.0609, AAB: 5.2176, AB: 4.7786, BA: 4.6728, BB: 5.0357\n",
      "  ERQ_ES: AAA: 4.0000, AAB: 2.5577, AB: 3.1094, BA: 3.1034, BB: 2.7679\n",
      "  UPPSP_NU: AAA: 7.6364, AAB: 7.6923, AB: 9.0625, BA: 9.3103, BB: 10.6429\n",
      "  UPPSP_PU: AAA: 9.4545, AAB: 9.0385, AB: 10.0938, BA: 9.8276, BB: 10.7857\n",
      "  UPPSP_SS: AAA: 10.4545, AAB: 11.7308, AB: 10.4688, BA: 11.4483, BB: 10.5714\n",
      "  UPPSP_PMD: AAA: 7.2727, AAB: 7.0769, AB: 7.1562, BA: 7.4138, BB: 9.2143\n",
      "  UPPSP_PSV: AAA: 7.9091, AAB: 5.9615, AB: 6.7969, BA: 7.5517, BB: 8.7143\n",
      "  BIS: AAA: 20.0000, AAB: 21.6538, AB: 22.3750, BA: 20.4138, BB: 21.9286\n",
      "  BAS_RR: AAA: 16.8182, AAB: 17.8077, AB: 17.6406, BA: 16.6207, BB: 18.0000\n",
      "  BAS_D: AAA: 10.9091, AAB: 12.2692, AB: 11.9375, BA: 11.3793, BB: 12.6429\n",
      "  BAS_FS: AAA: 11.9091, AAB: 12.4231, AB: 12.3750, BA: 12.5517, BB: 13.8571\n",
      "  TEPS_AF: AAA: 40.5455, AAB: 45.1538, AB: 42.6250, BA: 44.2069, BB: 43.5714\n",
      "  TEPS_CF: AAA: 34.5455, AAB: 38.7308, AB: 34.1094, BA: 36.4138, BB: 34.3571\n",
      "  SHS: AAA: 4.2273, AAB: 4.4327, AB: 4.1758, BA: 4.2500, BB: 4.4286\n",
      "  FS: AAA: 45.2727, AAB: 50.4231, AB: 46.4844, BA: 47.3448, BB: 48.1429\n",
      "  LOT_R: AAA: 14.1818, AAB: 17.2692, AB: 14.1875, BA: 13.8621, BB: 16.2143\n",
      "  RRQ_Rum: AAA: 3.4091, AAB: 2.9260, AB: 3.5419, BA: 3.1099, BB: 3.3521\n",
      "  RRQ_Ref: AAA: 3.7130, AAB: 3.2046, AB: 3.2089, BA: 3.3848, BB: 3.7740\n",
      "  ASI_P: AAA: 4.4545, AAB: 2.5000, AB: 5.6719, BA: 4.8621, BB: 4.5000\n",
      "  ASI_C: AAA: 3.8182, AAB: 1.4615, AB: 3.9688, BA: 3.5172, BB: 5.7857\n",
      "  ASI_S: AAA: 10.2727, AAB: 5.6923, AB: 9.2344, BA: 7.3793, BB: 8.5000\n",
      "  ASI_T: AAA: 18.5455, AAB: 9.6538, AB: 18.8750, BA: 15.7586, BB: 18.7857\n",
      "\n",
      "Configuration: Initial Feature=Dif_Match, Percentile=30, New Feature=Mean_Rating0_No_Match_Happy, New Percentile=35\n",
      "Cluster Sizes:\n",
      "  AAA: 11\n",
      "  AAB: 26\n",
      "  AB: 64\n",
      "  BA: 29\n",
      "  BB: 14\n",
      "Metric Means Across Clusters:\n",
      "  PA: AAA: 32.3636, AAB: 36.8846, AB: 31.9531, BA: 32.1724, BB: 34.2857\n",
      "  NA.: AAA: 20.5455, AAB: 18.2692, AB: 21.1250, BA: 20.0000, BB: 20.0000\n",
      "  ERQ_CR: AAA: 5.0609, AAB: 5.2176, AB: 4.7786, BA: 4.6728, BB: 5.0357\n",
      "  ERQ_ES: AAA: 4.0000, AAB: 2.5577, AB: 3.1094, BA: 3.1034, BB: 2.7679\n",
      "  UPPSP_NU: AAA: 7.6364, AAB: 7.6923, AB: 9.0625, BA: 9.3103, BB: 10.6429\n",
      "  UPPSP_PU: AAA: 9.4545, AAB: 9.0385, AB: 10.0938, BA: 9.8276, BB: 10.7857\n",
      "  UPPSP_SS: AAA: 10.4545, AAB: 11.7308, AB: 10.4688, BA: 11.4483, BB: 10.5714\n",
      "  UPPSP_PMD: AAA: 7.2727, AAB: 7.0769, AB: 7.1562, BA: 7.4138, BB: 9.2143\n",
      "  UPPSP_PSV: AAA: 7.9091, AAB: 5.9615, AB: 6.7969, BA: 7.5517, BB: 8.7143\n",
      "  BIS: AAA: 20.0000, AAB: 21.6538, AB: 22.3750, BA: 20.4138, BB: 21.9286\n",
      "  BAS_RR: AAA: 16.8182, AAB: 17.8077, AB: 17.6406, BA: 16.6207, BB: 18.0000\n",
      "  BAS_D: AAA: 10.9091, AAB: 12.2692, AB: 11.9375, BA: 11.3793, BB: 12.6429\n",
      "  BAS_FS: AAA: 11.9091, AAB: 12.4231, AB: 12.3750, BA: 12.5517, BB: 13.8571\n",
      "  TEPS_AF: AAA: 40.5455, AAB: 45.1538, AB: 42.6250, BA: 44.2069, BB: 43.5714\n",
      "  TEPS_CF: AAA: 34.5455, AAB: 38.7308, AB: 34.1094, BA: 36.4138, BB: 34.3571\n",
      "  SHS: AAA: 4.2273, AAB: 4.4327, AB: 4.1758, BA: 4.2500, BB: 4.4286\n",
      "  FS: AAA: 45.2727, AAB: 50.4231, AB: 46.4844, BA: 47.3448, BB: 48.1429\n",
      "  LOT_R: AAA: 14.1818, AAB: 17.2692, AB: 14.1875, BA: 13.8621, BB: 16.2143\n",
      "  RRQ_Rum: AAA: 3.4091, AAB: 2.9260, AB: 3.5419, BA: 3.1099, BB: 3.3521\n",
      "  RRQ_Ref: AAA: 3.7130, AAB: 3.2046, AB: 3.2089, BA: 3.3848, BB: 3.7740\n",
      "  ASI_P: AAA: 4.4545, AAB: 2.5000, AB: 5.6719, BA: 4.8621, BB: 4.5000\n",
      "  ASI_C: AAA: 3.8182, AAB: 1.4615, AB: 3.9688, BA: 3.5172, BB: 5.7857\n",
      "  ASI_S: AAA: 10.2727, AAB: 5.6923, AB: 9.2344, BA: 7.3793, BB: 8.5000\n",
      "  ASI_T: AAA: 18.5455, AAB: 9.6538, AB: 18.8750, BA: 15.7586, BB: 18.7857\n",
      "\n",
      "Configuration: Initial Feature=Dif_Match, Percentile=30, New Feature=Trend_Match, New Percentile=40\n",
      "Cluster Sizes:\n",
      "  AAA: 22\n",
      "  AAB: 15\n",
      "  ABA: 32\n",
      "  ABB: 32\n",
      "  BA: 32\n",
      "  BB: 11\n",
      "Metric Means Across Clusters:\n",
      "  PA: AAA: 33.2727, AAB: 38.8667, ABA: 32.8438, ABB: 31.0625, BA: 32.0938, BB: 35.0909\n",
      "  NA.: AAA: 18.5909, AAB: 19.4667, ABA: 19.8750, ABB: 22.3750, BA: 20.5938, BB: 18.2727\n",
      "  ERQ_CR: AAA: 4.9620, AAB: 5.4776, ABA: 4.6615, ABB: 4.8958, BA: 4.7605, BB: 4.8794\n",
      "  ERQ_ES: AAA: 3.0455, AAB: 2.9000, ABA: 2.8516, ABB: 3.3672, BA: 3.0938, BB: 2.7045\n",
      "  UPPSP_NU: AAA: 7.3636, AAB: 8.1333, ABA: 8.9688, ABB: 9.1562, BA: 10.4375, BB: 7.7273\n",
      "  UPPSP_PU: AAA: 9.0455, AAB: 9.3333, ABA: 10.3438, ABB: 9.8438, BA: 10.5938, BB: 8.8182\n",
      "  UPPSP_SS: AAA: 10.7273, AAB: 12.2667, ABA: 11.0312, ABB: 9.9062, BA: 11.3750, BB: 10.5455\n",
      "  UPPSP_PMD: AAA: 7.5000, AAB: 6.6000, ABA: 7.7188, ABB: 6.5938, BA: 8.3125, BB: 7.0909\n",
      "  UPPSP_PSV: AAA: 7.0909, AAB: 5.7333, ABA: 6.9062, ABB: 6.6875, BA: 8.3750, BB: 6.6364\n",
      "  BIS: AAA: 21.4091, AAB: 20.8000, ABA: 21.5625, ABB: 23.1875, BA: 21.1875, BB: 20.0909\n",
      "  BAS_RR: AAA: 16.7727, AAB: 18.6000, ABA: 17.8125, ABB: 17.4688, BA: 17.3125, BB: 16.3636\n",
      "  BAS_D: AAA: 11.2273, AAB: 12.8000, ABA: 12.1562, ABB: 11.7188, BA: 11.8750, BB: 11.5455\n",
      "  BAS_FS: AAA: 12.0909, AAB: 12.5333, ABA: 12.5000, ABB: 12.2500, BA: 13.3438, BB: 11.9091\n",
      "  TEPS_AF: AAA: 41.1818, AAB: 47.6000, ABA: 41.9688, ABB: 43.2812, BA: 45.0625, BB: 40.9091\n",
      "  TEPS_CF: AAA: 36.5000, AAB: 38.9333, ABA: 33.4375, ABB: 34.7812, BA: 36.4688, BB: 33.6364\n",
      "  SHS: AAA: 4.1705, AAB: 4.6667, ABA: 4.2109, ABB: 4.1406, BA: 4.3125, BB: 4.2955\n",
      "  FS: AAA: 48.0000, AAB: 50.2000, ABA: 47.4375, ABB: 45.5312, BA: 47.0312, BB: 49.2727\n",
      "  LOT_R: AAA: 16.3636, AAB: 16.3333, ABA: 14.7500, ABB: 13.6250, BA: 14.0312, BB: 16.3636\n",
      "  RRQ_Rum: AAA: 2.9164, AAB: 3.2944, ABA: 3.4772, ABB: 3.6067, BA: 3.3207, BB: 2.8048\n",
      "  RRQ_Ref: AAA: 3.2230, AAB: 3.5504, ABA: 3.0861, ABB: 3.3316, BA: 3.4661, BB: 3.6436\n",
      "  ASI_P: AAA: 2.8182, AAB: 3.4667, ABA: 5.1250, ABB: 6.2188, BA: 4.6250, BB: 5.0909\n",
      "  ASI_C: AAA: 2.0909, AAB: 2.2667, ABA: 2.9688, ABB: 4.9688, BA: 4.1875, BB: 4.4545\n",
      "  ASI_S: AAA: 6.6818, AAB: 7.6000, ABA: 7.3750, ABB: 11.0938, BA: 8.0312, BB: 6.9091\n",
      "  ASI_T: AAA: 11.5909, AAB: 13.3333, ABA: 15.4688, ABB: 22.2812, BA: 16.8438, BB: 16.4545\n",
      "\n",
      "Configuration: Initial Feature=Dif_Match, Percentile=30, New Feature=Trend_Match_Happy, New Percentile=35\n",
      "Cluster Sizes:\n",
      "  AAA: 22\n",
      "  AAB: 15\n",
      "  ABA: 38\n",
      "  ABB: 26\n",
      "  BA: 32\n",
      "  BB: 11\n",
      "Metric Means Across Clusters:\n",
      "  PA: AAA: 35.4091, AAB: 35.7333, ABA: 31.1316, ABB: 33.1538, BA: 31.9375, BB: 35.5455\n",
      "  NA.: AAA: 18.1818, AAB: 20.0667, ABA: 19.8684, ABB: 22.9615, BA: 20.9062, BB: 17.3636\n",
      "  ERQ_CR: AAA: 5.2421, AAB: 5.0667, ABA: 4.7107, ABB: 4.8779, BA: 4.7503, BB: 4.9091\n",
      "  ERQ_ES: AAA: 2.9432, AAB: 3.0500, ABA: 3.1842, ABB: 3.0000, BA: 3.1562, BB: 2.5227\n",
      "  UPPSP_NU: AAA: 7.7273, AAB: 7.6000, ABA: 8.5526, ABB: 9.8077, BA: 10.0938, BB: 8.7273\n",
      "  UPPSP_PU: AAA: 9.2727, AAB: 9.0000, ABA: 10.0789, ABB: 10.1154, BA: 10.4688, BB: 9.1818\n",
      "  UPPSP_SS: AAA: 12.2727, AAB: 10.0000, ABA: 10.2632, ABB: 10.7692, BA: 11.2188, BB: 11.0000\n",
      "  UPPSP_PMD: AAA: 7.2727, AAB: 6.9333, ABA: 7.2895, ABB: 6.9615, BA: 8.3750, BB: 6.9091\n",
      "  UPPSP_PSV: AAA: 6.5455, AAB: 6.5333, ABA: 6.8684, ABB: 6.6923, BA: 8.2500, BB: 7.0000\n",
      "  BIS: AAA: 21.2727, AAB: 21.0000, ABA: 21.7105, ABB: 23.3462, BA: 21.5312, BB: 19.0909\n",
      "  BAS_RR: AAA: 17.6818, AAB: 17.2667, ABA: 17.5526, ABB: 17.7692, BA: 17.1875, BB: 16.7273\n",
      "  BAS_D: AAA: 12.0000, AAB: 11.6667, ABA: 11.6316, ABB: 12.3846, BA: 12.0312, BB: 11.0909\n",
      "  BAS_FS: AAA: 12.7727, AAB: 11.5333, ABA: 12.1579, ABB: 12.6923, BA: 13.2812, BB: 12.0909\n",
      "  TEPS_AF: AAA: 43.2273, AAB: 44.6000, ABA: 40.8158, ABB: 45.2692, BA: 44.6562, BB: 42.0909\n",
      "  TEPS_CF: AAA: 36.2727, AAB: 39.2667, ABA: 32.2632, ABB: 36.8077, BA: 35.3438, BB: 36.9091\n",
      "  SHS: AAA: 4.3523, AAB: 4.4000, ABA: 4.0855, ABB: 4.3077, BA: 4.2344, BB: 4.5227\n",
      "  FS: AAA: 49.7727, AAB: 47.6000, ABA: 46.5526, ABB: 46.3846, BA: 47.1250, BB: 49.0000\n",
      "  LOT_R: AAA: 16.7727, AAB: 15.7333, ABA: 14.1053, ABB: 14.3077, BA: 13.8438, BB: 16.9091\n",
      "  RRQ_Rum: AAA: 3.1474, AAB: 2.9556, ABA: 3.4281, ABB: 3.7083, BA: 3.3753, BB: 2.6461\n",
      "  RRQ_Ref: AAA: 3.2039, AAB: 3.5784, ABA: 3.1254, ABB: 3.3308, BA: 3.4378, BB: 3.7261\n",
      "  ASI_P: AAA: 2.6364, AAB: 3.7333, ABA: 4.4737, ABB: 7.4231, BA: 5.3750, BB: 2.9091\n",
      "  ASI_C: AAA: 1.8636, AAB: 2.6000, ABA: 2.7105, ABB: 5.8077, BA: 5.1250, BB: 1.7273\n",
      "  ASI_S: AAA: 6.7727, AAB: 7.4667, ABA: 8.4211, ABB: 10.4231, BA: 8.6875, BB: 5.0000\n",
      "  ASI_T: AAA: 11.2727, AAB: 13.8000, ABA: 15.6053, ABB: 23.6538, BA: 19.1875, BB: 9.6364\n"
     ]
    }
   ],
   "source": [
    "analyze_top_configurations(final_results, feature_scales.copy(), metrics_columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T16:04:36.569102100Z",
     "start_time": "2024-07-14T16:04:36.306171Z"
    }
   },
   "id": "3df5c890958763fb",
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
