{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:15.100448400Z",
     "start_time": "2024-07-14T10:26:15.071543Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#We read the predictive processing data\n",
    "RETOS_BEBRASK_dataset = pd.read_excel(\"RETOS_BEBRASK_long.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:15.950685400Z",
     "start_time": "2024-07-14T10:26:15.278304400Z"
    }
   },
   "id": "bb4c4e20a6e36ff3",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def slope_intercept_regression(time_series):\n",
    "    \"\"\"\n",
    "    Perform linear regression on each row of a time series DataFrame to extract slopes and intercepts.\n",
    "\n",
    "    This function takes a time series DataFrame, where each row represents a subject and each column \n",
    "    represents a time point. It fits a linear regression model to the non-NaN values of each row \n",
    "    and returns the slopes and intercepts of the fitted lines.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (DataFrame): A pandas DataFrame where each row is a subject and each column is a time point.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two numpy arrays containing the slopes and intercepts for each subject.\n",
    "    \"\"\"\n",
    "    slopes = np.zeros(time_series.shape[0])\n",
    "    intercepts = np.zeros(time_series.shape[0])\n",
    "    \n",
    "    # Original time stamps as the independent variable\n",
    "    X_original = np.arange(1, time_series.shape[1] + 1)\n",
    "    \n",
    "    # Loop over each subject to fit a linear model and extract the slope\n",
    "    for i in range(time_series.shape[0]):\n",
    "        # Time series values for the current subject, dropping NaNs\n",
    "        Y = time_series.iloc[i, :].dropna().values\n",
    "    \n",
    "        # Ensure Y is of numeric type\n",
    "        Y = pd.to_numeric(Y, errors='coerce').astype('float64')\n",
    "    \n",
    "        # Filter X based on the non-NaN entries of Y to maintain correspondence\n",
    "        X_filtered = X_original[~time_series.iloc[i, :].isna()]\n",
    "    \n",
    "        # Ensure X_filtered is of numeric type\n",
    "        X_filtered = pd.to_numeric(X_filtered, errors='coerce').astype('float64')\n",
    "    \n",
    "        # Check again after conversion to avoid fitting a model with insufficient data\n",
    "        if len(Y) > 1 and not np.isnan(Y).all():\n",
    "            slope, intercept = np.polyfit(X_filtered, Y, 1)  # Fit a linear model\n",
    "            slopes[i] = slope  # Store the slope\n",
    "            intercepts[i] = intercept\n",
    "        else:\n",
    "            slopes[i] = np.nan  # Assign NaN if not enough data points or if conversion resulted in NaNs\n",
    "            intercepts[i] = np.nan\n",
    "    return slopes,intercepts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:15.967217300Z",
     "start_time": "2024-07-14T10:26:15.955685900Z"
    }
   },
   "id": "a7b1a1c2e5c0330c",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time series Features\n",
    "\n",
    "Getting info on predictability\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb206d7f956007f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract the subject IDs and time series data from the dataset\n",
    "subject_id = RETOS_BEBRASK_dataset.iloc[:, 0]\n",
    "subject_timeseries = RETOS_BEBRASK_dataset.iloc[:, 1:46]\n",
    "\n",
    "# Identify columns related to different types of data\n",
    "rating_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Rating0' in col]\n",
    "fulfilled_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Fulfilled' in col]\n",
    "emotions_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'EvokedEmotion' in col]\n",
    "\n",
    "# Get the index (subject IDs) of the dataset\n",
    "subjects = RETOS_BEBRASK_dataset.index\n",
    "\n",
    "# Initialize DataFrames to store ratings and fulfillment status over time for each subject\n",
    "\n",
    "# DataFrame for Rating 0 over 45 time points\n",
    "df_rating_0 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(45)])\n",
    "\n",
    "# DataFrames for Fulfilled status over 30 and 20 time points respectively (for margin of error)\n",
    "df_fulfilled_1 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(30)])\n",
    "df_fulfilled_0 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(20)])\n",
    "\n",
    "# DataFrames for ratings of specific emotions (happy, sad, fear) over 18 time points each (for margin of error)\n",
    "df_rating_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "df_rating_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "df_rating_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "\n",
    "# Initialize DataFrames for each emotion with 'Fulfilled' status over 11 time points (for margin of error)\n",
    "df_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "\n",
    "# Initialize DataFrames for each emotion with 'Not Fulfilled' status over 7 time points (for margin of error)\n",
    "df_no_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_no_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_no_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:16.013733900Z",
     "start_time": "2024-07-14T10:26:15.972217500Z"
    }
   },
   "id": "80393f33438c7287",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterate through each subject\n",
    "for subject in subjects:\n",
    "    ratings = []\n",
    "    # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0\n",
    "    ratings_1,ratings_0 = [], []\n",
    "        # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0 for each emotion\n",
    "    ratings_1_happy, ratings_1_sad, ratings_1_fear = [], [], []\n",
    "    ratings_0_happy, ratings_0_sad, ratings_0_fear = [], [], []\n",
    "    \n",
    "    ratings_happy, ratings_sad, ratings_fear = [], [], []\n",
    "\n",
    "\n",
    "    # Iterate through each 'Fulfilled' and 'Rating0' column pair\n",
    "    for fulfilled_col, rating_col, emotion_col in zip(fulfilled_columns, rating_columns, emotions_columns):\n",
    "        fulfilled_status = RETOS_BEBRASK_dataset.loc[subject, fulfilled_col]\n",
    "        rating_value = RETOS_BEBRASK_dataset.loc[subject, rating_col]\n",
    "        emotion_value = RETOS_BEBRASK_dataset.loc[subject, emotion_col]\n",
    "        ratings.append(rating_value)\n",
    "        if fulfilled_status == 1:\n",
    "            ratings_1.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                ratings_1_happy.append(rating_value)\n",
    "                ratings_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                ratings_1_sad.append(rating_value)\n",
    "                ratings_sad.append(rating_value)\n",
    "            \n",
    "            elif emotion_value == \"fear\":\n",
    "                ratings_1_fear.append(rating_value)\n",
    "                ratings_fear.append(rating_value)\n",
    "\n",
    "        elif fulfilled_status == 0:\n",
    "            ratings_0.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                ratings_0_happy.append(rating_value)\n",
    "                ratings_sad.append(rating_value)\n",
    "\n",
    "            elif emotion_value == \"sadness\":\n",
    "                ratings_0_sad.append(rating_value)\n",
    "                ratings_happy.append(rating_value)\n",
    "\n",
    "            elif emotion_value == \"fear\":\n",
    "                ratings_0_fear.append(rating_value)\n",
    "                ratings_fear.append(rating_value)\n",
    "\n",
    "    # Extend lists with NaN values if they are shorter than required lengths\n",
    "    \n",
    "    ratings_1.extend([np.nan] * (30 - len(ratings_1)))\n",
    "    ratings_0.extend([np.nan] * (20 - len(ratings_0)))\n",
    "\n",
    "    ratings_1_happy.extend([np.nan] * (11 - len(ratings_1_happy)))\n",
    "    ratings_1_sad.extend([np.nan] * (11 - len(ratings_1_sad)))\n",
    "    ratings_1_fear.extend([np.nan] * (11 - len(ratings_1_fear)))\n",
    "\n",
    "    ratings_0_happy.extend([np.nan] * (7 - len(ratings_0_happy)))\n",
    "    ratings_0_sad.extend([np.nan] * (7 - len(ratings_0_sad)))\n",
    "    ratings_0_fear.extend([np.nan] * (7 - len(ratings_0_fear)))\n",
    "    \n",
    "    ratings_happy.extend([np.nan] * (18 - len(ratings_happy)))\n",
    "    ratings_sad.extend([np.nan] * (18 - len(ratings_sad)))\n",
    "    ratings_fear.extend([np.nan] * (18 - len(ratings_fear)))\n",
    "\n",
    "    \n",
    "    # Insert lists into their respective DataFrames for the current subject\n",
    "    df_rating_0.loc[subject, :] = ratings\n",
    "    \n",
    "    df_fulfilled_1.loc[subject, :] = ratings_1\n",
    "    df_fulfilled_0.loc[subject, :] = ratings_0\n",
    "    \n",
    "    df_rating_happy.loc[subject, :] = ratings_happy\n",
    "    df_rating_sad.loc[subject, :] = ratings_sad\n",
    "    df_rating_fear.loc[subject, :] = ratings_fear\n",
    "\n",
    "    df_fulfilled_happy.loc[subject, :] = ratings_1_happy\n",
    "    df_fulfilled_sad.loc[subject, :] = ratings_1_sad\n",
    "    df_fulfilled_fear.loc[subject, :] = ratings_1_fear\n",
    "\n",
    "    df_no_fulfilled_happy.loc[subject, :] = ratings_0_happy\n",
    "    df_no_fulfilled_sad.loc[subject, :] = ratings_0_sad\n",
    "    df_no_fulfilled_fear.loc[subject, :] = ratings_0_fear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:16.575816500Z",
     "start_time": "2024-07-14T10:26:16.090633800Z"
    }
   },
   "id": "e10999bd129b9f21",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract the subjects' identifiers\n",
    "subjects = RETOS_BEBRASK_dataset[\"DataFile.Basename\"]\n",
    "\n",
    "# Perform linear regression on fulfilled data (df_fulfilled_1 and df_fulfilled_0)\n",
    "slopes_match, intercepts_match = slope_intercept_regression(df_fulfilled_1)\n",
    "slopes_no_match, intercepts_no_match = slope_intercept_regression(df_fulfilled_0)\n",
    "\n",
    "# Perform linear regression on fulfilled 'happy' emotion data\n",
    "slopes_match_happy, intercepts_match_happy = slope_intercept_regression(df_fulfilled_happy)\n",
    "slopes_match_no_happy, intercepts_match_no_happy = slope_intercept_regression(df_no_fulfilled_happy)\n",
    "\n",
    "# Perform linear regression on fulfilled 'sad' emotion data\n",
    "slopes_match_sad, intercepts_match_sad = slope_intercept_regression(df_fulfilled_sad)\n",
    "slopes_match_no_sad, intercepts_match_no_sad = slope_intercept_regression(df_no_fulfilled_sad)\n",
    "\n",
    "# Perform linear regression on fulfilled 'fear' emotion data\n",
    "slopes_match_fear, intercepts_match_fear = slope_intercept_regression(df_fulfilled_fear)\n",
    "slopes_match_no_fear, intercepts_match_no_fear = slope_intercept_regression(df_no_fulfilled_fear)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:17.380796400Z",
     "start_time": "2024-07-14T10:26:16.554255900Z"
    }
   },
   "id": "71aeb527d99982f8",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a DataFrame from the computed slopes and intercepts for different conditions and emotions\n",
    "time_df = pd.DataFrame([\n",
    "    subjects,\n",
    "    slopes_match,\n",
    "    slopes_no_match,\n",
    "    intercepts_match,\n",
    "    intercepts_no_match,\n",
    "    slopes_match_happy,\n",
    "    slopes_match_no_happy,\n",
    "    intercepts_match_happy,\n",
    "    intercepts_match_no_happy,\n",
    "    slopes_match_sad,\n",
    "    slopes_match_no_sad,\n",
    "    intercepts_match_sad,\n",
    "    intercepts_match_no_sad,\n",
    "    slopes_match_fear,\n",
    "    slopes_match_no_fear,\n",
    "    intercepts_match_fear,\n",
    "    intercepts_match_no_fear,\n",
    "    (slopes_match_sad + slopes_match_fear) / 2,  # Average slope for sad and fear (negative emotions)\n",
    "    (slopes_match_no_sad + slopes_match_no_fear) / 2,  # Average slope for sad and fear (negative emotions) with no match\n",
    "    (intercepts_match_sad + intercepts_match_fear) / 2,  # Average intercept for sad and fear (negative emotions)\n",
    "    (intercepts_match_no_fear + intercepts_match_no_sad) / 2  # Average intercept for sad and fear (negative emotions) with no match\n",
    "]).transpose()\n",
    "\n",
    "# Assign column names to the DataFrame\n",
    "time_df.columns = [\n",
    "    \"Subject\",\n",
    "    \"Trend_Match\",\n",
    "    \"Trend_No_Match\",\n",
    "    \"Intercept_Match\",\n",
    "    \"Intercept_No_Match\",\n",
    "    \"Trend_Match_Happy\",\n",
    "    \"Trend_No_Match_Happy\",\n",
    "    \"Intercept_Match_Happy\",\n",
    "    \"Intercept_No_Match_Happy\",\n",
    "    \"Trend_Match_Sad\",\n",
    "    \"Trend_No_Match_Sad\",\n",
    "    \"Intercept_Match_Sad\",\n",
    "    \"Intercept_No_Match_Sad\",\n",
    "    \"Trend_Match_Fear\",\n",
    "    \"Trend_No_Match_Fear\",\n",
    "    \"Intercept_Match_Fear\",\n",
    "    \"Intercept_No_Match_Fear\",\n",
    "    \"Trend_Match_Negative\",  # Average slope for sad and fear (negative emotions)\n",
    "    \"Trend_No_Match_Negative\",  # Average slope for sad and fear (negative emotions) with no match\n",
    "    \"Intercept_Match_Negative\",  # Average intercept for sad and fear (negative emotions)\n",
    "    \"Intercept_No_Match_Negative\"  # Average intercept for sad and fear (negative emotions) with no match\n",
    "]\n",
    "\n",
    "# Calculate final values at specific time points for match and no match conditions\n",
    "time_df[\"Final_Value_Match\"] = time_df[\"Intercept_Match\"] + time_df[\"Trend_Match\"] * 27\n",
    "time_df[\"Final_Value_No_Match\"] = time_df[\"Intercept_No_Match\"] + time_df[\"Trend_No_Match\"] * 18\n",
    "\n",
    "# Calculate final values for happy emotion at specific time points for match and no match conditions\n",
    "time_df[\"Final_Value_Match_Happy\"] = time_df[\"Intercept_Match_Happy\"] + time_df[\"Trend_Match_Happy\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Happy\"] = time_df[\"Intercept_No_Match_Happy\"] + time_df[\"Trend_No_Match_Happy\"] * 6\n",
    "\n",
    "# Calculate final values for sad emotion at specific time points for match and no match conditions\n",
    "time_df[\"Final_Value_Match_Sad\"] = time_df[\"Intercept_Match_Sad\"] + time_df[\"Trend_Match_Sad\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Sad\"] = time_df[\"Intercept_No_Match_Sad\"] + time_df[\"Trend_No_Match_Sad\"] * 6\n",
    "\n",
    "# Calculate final values for fear emotion at specific time points for match and no match conditions\n",
    "time_df[\"Final_Value_Match_Fear\"] = time_df[\"Intercept_Match_Fear\"] + time_df[\"Trend_Match_Fear\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Fear\"] = time_df[\"Intercept_No_Match_Fear\"] + time_df[\"Trend_No_Match_Fear\"] * 6\n",
    "\n",
    "# Calculate final values for negative emotions (average of sad and fear) at specific time points for match and no match conditions\n",
    "time_df[\"Final_Value_Match_Negative\"] = time_df[\"Intercept_Match_Negative\"] + time_df[\"Trend_Match_Negative\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Negative\"] = time_df[\"Intercept_No_Match_Negative\"] + time_df[\"Trend_No_Match_Negative\"] * 6\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:17.416014300Z",
     "start_time": "2024-07-14T10:26:17.393834700Z"
    }
   },
   "id": "c8bb310b81d87c2c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "time_df.to_excel('../Clustering_Predictive_Processing/trend_dataset.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:17.631273700Z",
     "start_time": "2024-07-14T10:26:17.414507700Z"
    }
   },
   "id": "c3a4a4e698ce4ff",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theory Driven Features\n",
    "Getting info on likeability\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24f17cbeaf61f512"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identify columns related to different types of data\n",
    "fulfilled_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Fulfilled' in col]\n",
    "emotions_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'EvokedEmotion' in col]\n",
    "likeability_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Rating.' in col]\n",
    "\n",
    "# Get the index (subject IDs) of the dataset\n",
    "subjects = RETOS_BEBRASK_dataset.index\n",
    "\n",
    "# Initialize DataFrames to store likeability ratings over time for each subject\n",
    "\n",
    "# DataFrame for Likeability ratings over 45 time points\n",
    "df_likeability = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(45)])\n",
    "\n",
    "# DataFrames for Likeability ratings with fulfilled and not fulfilled statuses over 30 and 20 time points respectively\n",
    "df_likeability_fulfilled = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(30)])\n",
    "df_likeability_no_fulfilled = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(20)])\n",
    "\n",
    "# DataFrames for Likeability ratings of specific emotions (happy, sad, fear) over 18 time points each\n",
    "df_likeability_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "df_likeability_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "df_likeability_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(18)])\n",
    "\n",
    "# Initialize DataFrames for Likeability ratings of each emotion with 'Fulfilled' status over 11 time points\n",
    "df_likeability_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_likeability_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_likeability_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "\n",
    "# Initialize DataFrames for Likeability ratings of each emotion with 'Not Fulfilled' status over 7 time points\n",
    "df_likeability_no_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_likeability_no_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_likeability_no_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:17.721003900Z",
     "start_time": "2024-07-14T10:26:17.641113500Z"
    }
   },
   "id": "fdff4dd16d1f6eee",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterate through each subject\n",
    "for subject in subjects:\n",
    "    likeability = []\n",
    "    # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0\n",
    "    likeability_1, likeability_0 = [], []\n",
    "    # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0 for each emotion\n",
    "    likeability_1_happy, likeability_1_sad, likeability_1_fear = [], [], []\n",
    "    likeability_0_happy, likeability_0_sad, likeability_0_fear = [], [], []\n",
    "    likeability_happy, likeability_sad, likeability_fear = [], [], []\n",
    "\n",
    "    # Iterate through each 'Fulfilled' and 'Rating0' column pair\n",
    "    for fulfilled_col, likeability_col, emotion_col in zip(fulfilled_columns, likeability_columns, emotions_columns):\n",
    "        fulfilled_status = RETOS_BEBRASK_dataset.loc[subject, fulfilled_col]\n",
    "        rating_value = RETOS_BEBRASK_dataset.loc[subject, likeability_col]\n",
    "        emotion_value = RETOS_BEBRASK_dataset.loc[subject, emotion_col]\n",
    "        likeability.append(rating_value)\n",
    "        if fulfilled_status == 1:\n",
    "            likeability_1.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                likeability_1_happy.append(rating_value)\n",
    "                likeability_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                likeability_1_sad.append(rating_value)\n",
    "                likeability_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                likeability_1_fear.append(rating_value)\n",
    "                likeability_fear.append(rating_value)\n",
    "        elif fulfilled_status == 0:\n",
    "            likeability_0.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                likeability_0_happy.append(rating_value)\n",
    "                likeability_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                likeability_0_sad.append(rating_value)\n",
    "                likeability_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                likeability_0_fear.append(rating_value)\n",
    "                likeability_fear.append(rating_value)\n",
    "\n",
    "    # Extend lists with NaN values if they are shorter than required lengths\n",
    "\n",
    "    likeability_1.extend([np.nan] * (30 - len(likeability_1)))\n",
    "    likeability_0.extend([np.nan] * (20 - len(likeability_0)))\n",
    "\n",
    "    likeability_1_happy.extend([np.nan] * (11 - len(likeability_1_happy)))\n",
    "    likeability_1_sad.extend([np.nan] * (11 - len(likeability_1_sad)))\n",
    "    likeability_1_fear.extend([np.nan] * (11 - len(likeability_1_fear)))\n",
    "\n",
    "    likeability_happy.extend([np.nan] * (18 - len(likeability_happy)))\n",
    "    likeability_sad.extend([np.nan] * (18 - len(likeability_sad)))\n",
    "    likeability_fear.extend([np.nan] * (18 - len(likeability_fear)))\n",
    "\n",
    "    likeability_0_happy.extend([np.nan] * (7 - len(likeability_0_happy)))\n",
    "    likeability_0_sad.extend([np.nan] * (7 - len(likeability_0_sad)))\n",
    "    likeability_0_fear.extend([np.nan] * (7 - len(likeability_0_fear)))\n",
    "\n",
    "    # Insert lists into their respective DataFrames for the current subject\n",
    "    df_likeability.loc[subject, :] = likeability\n",
    "\n",
    "    df_likeability_happy.loc[subject, :] = likeability_happy\n",
    "    df_likeability_sad.loc[subject, :] = likeability_sad\n",
    "    df_likeability_fear.loc[subject, :] = likeability_fear\n",
    "\n",
    "    df_likeability_fulfilled.loc[subject, :] = likeability_1\n",
    "    df_likeability_no_fulfilled.loc[subject, :] = likeability_0\n",
    "\n",
    "    df_likeability_fulfilled_happy.loc[subject, :] = likeability_1_happy\n",
    "    df_likeability_fulfilled_sad.loc[subject, :] = likeability_1_sad\n",
    "    df_likeability_fulfilled_fear.loc[subject, :] = likeability_1_fear\n",
    "\n",
    "    df_likeability_no_fulfilled_happy.loc[subject, :] = likeability_0_happy\n",
    "    df_likeability_no_fulfilled_sad.loc[subject, :] = likeability_0_sad\n",
    "    df_likeability_no_fulfilled_fear.loc[subject, :] = likeability_0_fear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:18.130664200Z",
     "start_time": "2024-07-14T10:26:17.725704100Z"
    }
   },
   "id": "ae681bedd65a63a5",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Correlations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "822c52546db2c1eb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manue\\anaconda3\\envs\\CASLabv2\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\manue\\anaconda3\\envs\\CASLabv2\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Cor_Pred_Like  Cor_Pred_Like_Match  Cor_Pred_Like_No_Match  \\\n0         0.113495             0.209359                0.103807   \n1         0.116469             0.291056               -0.051510   \n2         0.250345             0.432551                0.139859   \n3        -0.230171            -0.006600               -0.585369   \n4         0.034141             0.294244               -0.409852   \n..             ...                  ...                     ...   \n145       0.255697             0.281127                0.260715   \n146       0.607126             0.418214                0.346488   \n147       0.438726             0.514426                0.000000   \n148       0.366738             0.486755               -0.235897   \n149       0.606272             0.512718                0.582209   \n\n     Cor_Pred_Like_Match_Happy  Cor_Pred_Like_No_Match_Happy  \\\n0                -2.500000e-01                      0.542326   \n1                 4.902903e-01                     -0.632456   \n2                -2.294157e-01                      0.800000   \n3                 1.409710e-02                      0.000000   \n4                 5.345225e-01                      0.375000   \n..                         ...                           ...   \n145               9.433333e-02                      0.727607   \n146               2.467162e-17                     -0.108465   \n147               7.765164e-01                      0.000000   \n148               1.000000e+00                      0.612372   \n149               3.611576e-01                      0.000000   \n\n     Cor_Pred_Like_Match_Sad  Cor_Pred_Like_No_Match_Sad  \\\n0                   0.342381                   -0.061430   \n1                  -0.292770                    0.000000   \n2                   0.230940                    0.000000   \n3                   0.000000                   -0.542326   \n4                  -0.774597                   -0.875000   \n..                       ...                         ...   \n145                -0.036564                    0.342997   \n146                -0.142857                    0.542326   \n147                 0.346423                   -0.121566   \n148                 0.000000                   -0.801784   \n149                 0.288675                    0.000000   \n\n     Cor_Pred_Like_Match_Fear  Cor_Pred_Like_No_Match_Fear  \\\n0                    0.196722                    -0.188982   \n1                    0.258199                     0.000000   \n2                    0.307692                    -0.307148   \n3                   -0.280000                    -0.131306   \n4                   -0.762493                    -0.867722   \n..                        ...                          ...   \n145                  0.463586                    -0.316228   \n146                  0.304997                     0.242536   \n147                  0.548795                    -0.242536   \n148                  0.202031                    -0.534522   \n149                  0.363774                     0.801784   \n\n     Cor_Pred_Like_Match_Negative  Cor_Pred_Like_No_Match_Negative  \\\n0                        0.269552                        -0.125206   \n1                       -0.017286                         0.000000   \n2                        0.269316                        -0.153574   \n3                       -0.140000                        -0.336816   \n4                       -0.768545                        -0.871361   \n..                            ...                              ...   \n145                      0.213511                         0.013385   \n146                      0.081070                         0.392431   \n147                      0.447609                        -0.182051   \n148                      0.101015                        -0.668153   \n149                      0.326224                         0.400892   \n\n     Cor_Pred_Like_Happy  Cor_Pred_Like_Sad  Cor_Pred_Like_Fear  \\\n0               0.517809          -0.111746           -0.010479   \n1              -0.129387           0.024807            0.053548   \n2               0.489720          -0.235056           -0.251638   \n3               0.143346          -0.041910           -0.308632   \n4               0.534183           0.215294           -0.781323   \n..                   ...                ...                 ...   \n145            -0.159448          -0.512930            0.311421   \n146             0.348972           0.170523            0.320256   \n147             0.667781          -0.015347            0.132019   \n148             0.173448          -0.544581            0.108465   \n149            -0.010329          -0.433013            0.532909   \n\n     Cor_Pred_Like_Negative  \n0                 -0.061112  \n1                  0.039177  \n2                 -0.243347  \n3                 -0.175271  \n4                 -0.283014  \n..                      ...  \n145               -0.100755  \n146                0.245390  \n147                0.058336  \n148               -0.218058  \n149                0.049948  \n\n[150 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cor_Pred_Like</th>\n      <th>Cor_Pred_Like_Match</th>\n      <th>Cor_Pred_Like_No_Match</th>\n      <th>Cor_Pred_Like_Match_Happy</th>\n      <th>Cor_Pred_Like_No_Match_Happy</th>\n      <th>Cor_Pred_Like_Match_Sad</th>\n      <th>Cor_Pred_Like_No_Match_Sad</th>\n      <th>Cor_Pred_Like_Match_Fear</th>\n      <th>Cor_Pred_Like_No_Match_Fear</th>\n      <th>Cor_Pred_Like_Match_Negative</th>\n      <th>Cor_Pred_Like_No_Match_Negative</th>\n      <th>Cor_Pred_Like_Happy</th>\n      <th>Cor_Pred_Like_Sad</th>\n      <th>Cor_Pred_Like_Fear</th>\n      <th>Cor_Pred_Like_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.113495</td>\n      <td>0.209359</td>\n      <td>0.103807</td>\n      <td>-2.500000e-01</td>\n      <td>0.542326</td>\n      <td>0.342381</td>\n      <td>-0.061430</td>\n      <td>0.196722</td>\n      <td>-0.188982</td>\n      <td>0.269552</td>\n      <td>-0.125206</td>\n      <td>0.517809</td>\n      <td>-0.111746</td>\n      <td>-0.010479</td>\n      <td>-0.061112</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.116469</td>\n      <td>0.291056</td>\n      <td>-0.051510</td>\n      <td>4.902903e-01</td>\n      <td>-0.632456</td>\n      <td>-0.292770</td>\n      <td>0.000000</td>\n      <td>0.258199</td>\n      <td>0.000000</td>\n      <td>-0.017286</td>\n      <td>0.000000</td>\n      <td>-0.129387</td>\n      <td>0.024807</td>\n      <td>0.053548</td>\n      <td>0.039177</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.250345</td>\n      <td>0.432551</td>\n      <td>0.139859</td>\n      <td>-2.294157e-01</td>\n      <td>0.800000</td>\n      <td>0.230940</td>\n      <td>0.000000</td>\n      <td>0.307692</td>\n      <td>-0.307148</td>\n      <td>0.269316</td>\n      <td>-0.153574</td>\n      <td>0.489720</td>\n      <td>-0.235056</td>\n      <td>-0.251638</td>\n      <td>-0.243347</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.230171</td>\n      <td>-0.006600</td>\n      <td>-0.585369</td>\n      <td>1.409710e-02</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.542326</td>\n      <td>-0.280000</td>\n      <td>-0.131306</td>\n      <td>-0.140000</td>\n      <td>-0.336816</td>\n      <td>0.143346</td>\n      <td>-0.041910</td>\n      <td>-0.308632</td>\n      <td>-0.175271</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.034141</td>\n      <td>0.294244</td>\n      <td>-0.409852</td>\n      <td>5.345225e-01</td>\n      <td>0.375000</td>\n      <td>-0.774597</td>\n      <td>-0.875000</td>\n      <td>-0.762493</td>\n      <td>-0.867722</td>\n      <td>-0.768545</td>\n      <td>-0.871361</td>\n      <td>0.534183</td>\n      <td>0.215294</td>\n      <td>-0.781323</td>\n      <td>-0.283014</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.255697</td>\n      <td>0.281127</td>\n      <td>0.260715</td>\n      <td>9.433333e-02</td>\n      <td>0.727607</td>\n      <td>-0.036564</td>\n      <td>0.342997</td>\n      <td>0.463586</td>\n      <td>-0.316228</td>\n      <td>0.213511</td>\n      <td>0.013385</td>\n      <td>-0.159448</td>\n      <td>-0.512930</td>\n      <td>0.311421</td>\n      <td>-0.100755</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.607126</td>\n      <td>0.418214</td>\n      <td>0.346488</td>\n      <td>2.467162e-17</td>\n      <td>-0.108465</td>\n      <td>-0.142857</td>\n      <td>0.542326</td>\n      <td>0.304997</td>\n      <td>0.242536</td>\n      <td>0.081070</td>\n      <td>0.392431</td>\n      <td>0.348972</td>\n      <td>0.170523</td>\n      <td>0.320256</td>\n      <td>0.245390</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.438726</td>\n      <td>0.514426</td>\n      <td>0.000000</td>\n      <td>7.765164e-01</td>\n      <td>0.000000</td>\n      <td>0.346423</td>\n      <td>-0.121566</td>\n      <td>0.548795</td>\n      <td>-0.242536</td>\n      <td>0.447609</td>\n      <td>-0.182051</td>\n      <td>0.667781</td>\n      <td>-0.015347</td>\n      <td>0.132019</td>\n      <td>0.058336</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.366738</td>\n      <td>0.486755</td>\n      <td>-0.235897</td>\n      <td>1.000000e+00</td>\n      <td>0.612372</td>\n      <td>0.000000</td>\n      <td>-0.801784</td>\n      <td>0.202031</td>\n      <td>-0.534522</td>\n      <td>0.101015</td>\n      <td>-0.668153</td>\n      <td>0.173448</td>\n      <td>-0.544581</td>\n      <td>0.108465</td>\n      <td>-0.218058</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.606272</td>\n      <td>0.512718</td>\n      <td>0.582209</td>\n      <td>3.611576e-01</td>\n      <td>0.000000</td>\n      <td>0.288675</td>\n      <td>0.000000</td>\n      <td>0.363774</td>\n      <td>0.801784</td>\n      <td>0.326224</td>\n      <td>0.400892</td>\n      <td>-0.010329</td>\n      <td>-0.433013</td>\n      <td>0.532909</td>\n      <td>0.049948</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_correlation(row1, row2):\n",
    "    \"\"\"\n",
    "    Calculate the correlation between two rows, handling constant data.\n",
    "\n",
    "    If the standard deviation of either row is zero, it returns 1 if the rows are identical,\n",
    "    otherwise returns 0. If both rows have non-zero standard deviations, it returns the\n",
    "    Pearson correlation coefficient.\n",
    "\n",
    "    Parameters:\n",
    "    row1 (Series): The first row.\n",
    "    row2 (Series): The second row.\n",
    "\n",
    "    Returns:\n",
    "    float: The correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Check if the standard deviation is zero\n",
    "    if row1.std() == 0 or row2.std() == 0:\n",
    "        # Handle the constant data case here\n",
    "        return 1 if row1.equals(row2) else 0\n",
    "    return row1.corr(row2)\n",
    "\n",
    "# Calculate correlations for various conditions and datasets\n",
    "correlations_all = [row_correlation(df_likeability.loc[index], df_rating_0.loc[index]) for index in df_likeability.index]\n",
    "correlations_match = [row_correlation(df_likeability_fulfilled.loc[index], df_fulfilled_1.loc[index]) for index in df_likeability_fulfilled.index]\n",
    "correlations_no_match = [row_correlation(df_likeability_no_fulfilled.loc[index], df_fulfilled_0.loc[index]) for index in df_likeability_no_fulfilled.index]\n",
    "correlations_happy_match = [row_correlation(df_likeability_fulfilled_happy.loc[index], df_fulfilled_happy.loc[index]) for index in df_likeability_fulfilled_happy.index]\n",
    "correlations_happy_no_match = [row_correlation(df_likeability_no_fulfilled_happy.loc[index], df_no_fulfilled_happy.loc[index]) for index in df_likeability_no_fulfilled_happy.index]\n",
    "correlations_sad_match = [row_correlation(df_likeability_fulfilled_sad.loc[index], df_fulfilled_sad.loc[index]) for index in df_likeability_fulfilled_sad.index]\n",
    "correlations_sad_no_match = [row_correlation(df_likeability_no_fulfilled_sad.loc[index], df_no_fulfilled_sad.loc[index]) for index in df_likeability_no_fulfilled_sad.index]\n",
    "correlations_fear_match = [row_correlation(df_likeability_fulfilled_fear.loc[index], df_fulfilled_fear.loc[index]) for index in df_likeability_fulfilled_fear.index]\n",
    "correlations_fear_no_match = [row_correlation(df_likeability_no_fulfilled_fear.loc[index], df_no_fulfilled_fear.loc[index]) for index in df_likeability_no_fulfilled_fear.index]\n",
    "correlations_happy = [row_correlation(df_likeability_happy.loc[index], df_rating_happy.loc[index]) for index in df_likeability_happy.index]\n",
    "correlations_sad = [row_correlation(df_likeability_sad.loc[index], df_rating_sad.loc[index]) for index in df_likeability_sad.index]\n",
    "correlations_fear = [row_correlation(df_likeability_fear.loc[index], df_rating_fear.loc[index]) for index in df_likeability_fear.index]\n",
    "\n",
    "# Create a DataFrame to store all correlation results\n",
    "correlation_df = pd.DataFrame([\n",
    "    correlations_all,\n",
    "    correlations_match,\n",
    "    correlations_no_match,\n",
    "    correlations_happy_match,\n",
    "    correlations_happy_no_match,\n",
    "    correlations_sad_match,\n",
    "    correlations_sad_no_match,\n",
    "    correlations_fear_match,\n",
    "    correlations_fear_no_match,\n",
    "    (np.array(correlations_sad_match) + np.array(correlations_fear_match)) / 2,  # Average correlation for sad and fear match\n",
    "    (np.array(correlations_sad_no_match) + np.array(correlations_fear_no_match)) / 2,  # Average correlation for sad and fear no match\n",
    "    correlations_happy,\n",
    "    correlations_sad,\n",
    "    correlations_fear,\n",
    "    (np.array(correlations_sad) + np.array(correlations_fear)) / 2  # Average correlation for sad and fear\n",
    "]).transpose()\n",
    "\n",
    "# Assign column names to the correlation DataFrame\n",
    "correlation_df.columns = [\n",
    "    \"Cor_Pred_Like\",\n",
    "    \"Cor_Pred_Like_Match\",\n",
    "    \"Cor_Pred_Like_No_Match\",\n",
    "    \"Cor_Pred_Like_Match_Happy\",\n",
    "    \"Cor_Pred_Like_No_Match_Happy\",\n",
    "    \"Cor_Pred_Like_Match_Sad\",\n",
    "    \"Cor_Pred_Like_No_Match_Sad\",\n",
    "    \"Cor_Pred_Like_Match_Fear\",\n",
    "    \"Cor_Pred_Like_No_Match_Fear\",\n",
    "    \"Cor_Pred_Like_Match_Negative\",\n",
    "    \"Cor_Pred_Like_No_Match_Negative\",\n",
    "    \"Cor_Pred_Like_Happy\",\n",
    "    \"Cor_Pred_Like_Sad\",\n",
    "    \"Cor_Pred_Like_Fear\",\n",
    "    \"Cor_Pred_Like_Negative\"\n",
    "]\n",
    "\n",
    "# Display the correlation DataFrame\n",
    "correlation_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:19.224865100Z",
     "start_time": "2024-07-14T10:26:18.141186600Z"
    }
   },
   "id": "5f0ea0d3aa599a84",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Average Rating"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f61249fcb5dcc1e0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Mean_Rating0  Mean_Rating0_Match  Mean_Rating0_No_Match  \\\n0        2.888889            3.555556               1.888889   \n1        2.533333            3.000000               1.833333   \n2        2.311111            3.000000               1.277778   \n3        2.555556            3.148148               1.666667   \n4        2.266667            2.851852               1.388889   \n..            ...                 ...                    ...   \n145      2.088889            2.555556               1.388889   \n146      2.409091            3.076923               1.444444   \n147      2.613636            3.230769               1.722222   \n148      2.590909            3.222222               1.588235   \n149      1.933333            2.444444               1.166667   \n\n     Mean_Rating0_Match_Happy  Mean_Rating0_No_Match_Happy  \\\n0                    3.888889                     1.500000   \n1                    3.333333                     1.333333   \n2                    3.555556                     1.166667   \n3                    3.222222                     1.000000   \n4                    3.666667                     1.166667   \n..                        ...                          ...   \n145                  3.111111                     1.500000   \n146                  3.666667                     1.166667   \n147                  3.625000                     1.000000   \n148                  3.777778                     1.333333   \n149                  3.000000                     1.000000   \n\n     Mean_Rating0_Match_Sad  Mean_Rating0_No_Match_Sad  \\\n0                  3.555556                   2.166667   \n1                  2.555556                   1.833333   \n2                  2.666667                   1.500000   \n3                  3.000000                   2.666667   \n4                  2.666667                   1.333333   \n..                      ...                        ...   \n145                2.111111                   1.333333   \n146                2.750000                   1.333333   \n147                2.888889                   1.833333   \n148                3.000000                   1.666667   \n149                2.444444                   1.000000   \n\n     Mean_Rating0_Match_Fear  Mean_Rating0_No_Match_Fear  \\\n0                   3.222222                    2.000000   \n1                   3.111111                    2.333333   \n2                   2.777778                    1.166667   \n3                   3.222222                    1.333333   \n4                   2.222222                    1.666667   \n..                       ...                         ...   \n145                 2.444444                    1.333333   \n146                 2.777778                    1.833333   \n147                 3.222222                    2.333333   \n148                 2.888889                    1.800000   \n149                 1.888889                    1.500000   \n\n     Mean_Rating0_Match_Negative  Mean_Rating0_No_Match_Negative  \\\n0                       3.388889                        2.083333   \n1                       2.833333                        2.083333   \n2                       2.722222                        1.333333   \n3                       3.111111                        2.000000   \n4                       2.444444                        1.500000   \n..                           ...                             ...   \n145                     2.277778                        1.333333   \n146                     2.763889                        1.583333   \n147                     3.055556                        2.083333   \n148                     2.944444                        1.733333   \n149                     2.166667                        1.250000   \n\n     Mean_Rating0_Happy  Mean_Rating0_Sad  Mean_Rating0_Fear  \\\n0              3.200000          2.733333           2.733333   \n1              2.733333          2.066667           2.800000   \n2              2.733333          2.066667           2.133333   \n3              3.000000          2.200000           2.466667   \n4              2.733333          2.066667           2.000000   \n..                  ...               ...                ...   \n145            2.400000          1.866667           2.000000   \n146            2.733333          2.071429           2.400000   \n147            2.857143          2.133333           2.866667   \n148            2.933333          2.333333           2.500000   \n149            2.200000          1.866667           1.733333   \n\n     Mean_Rating0_Negative  \n0                 2.733333  \n1                 2.433333  \n2                 2.100000  \n3                 2.333333  \n4                 2.033333  \n..                     ...  \n145               1.933333  \n146               2.235714  \n147               2.500000  \n148               2.416667  \n149               1.800000  \n\n[150 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean_Rating0</th>\n      <th>Mean_Rating0_Match</th>\n      <th>Mean_Rating0_No_Match</th>\n      <th>Mean_Rating0_Match_Happy</th>\n      <th>Mean_Rating0_No_Match_Happy</th>\n      <th>Mean_Rating0_Match_Sad</th>\n      <th>Mean_Rating0_No_Match_Sad</th>\n      <th>Mean_Rating0_Match_Fear</th>\n      <th>Mean_Rating0_No_Match_Fear</th>\n      <th>Mean_Rating0_Match_Negative</th>\n      <th>Mean_Rating0_No_Match_Negative</th>\n      <th>Mean_Rating0_Happy</th>\n      <th>Mean_Rating0_Sad</th>\n      <th>Mean_Rating0_Fear</th>\n      <th>Mean_Rating0_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.888889</td>\n      <td>3.555556</td>\n      <td>1.888889</td>\n      <td>3.888889</td>\n      <td>1.500000</td>\n      <td>3.555556</td>\n      <td>2.166667</td>\n      <td>3.222222</td>\n      <td>2.000000</td>\n      <td>3.388889</td>\n      <td>2.083333</td>\n      <td>3.200000</td>\n      <td>2.733333</td>\n      <td>2.733333</td>\n      <td>2.733333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.533333</td>\n      <td>3.000000</td>\n      <td>1.833333</td>\n      <td>3.333333</td>\n      <td>1.333333</td>\n      <td>2.555556</td>\n      <td>1.833333</td>\n      <td>3.111111</td>\n      <td>2.333333</td>\n      <td>2.833333</td>\n      <td>2.083333</td>\n      <td>2.733333</td>\n      <td>2.066667</td>\n      <td>2.800000</td>\n      <td>2.433333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.311111</td>\n      <td>3.000000</td>\n      <td>1.277778</td>\n      <td>3.555556</td>\n      <td>1.166667</td>\n      <td>2.666667</td>\n      <td>1.500000</td>\n      <td>2.777778</td>\n      <td>1.166667</td>\n      <td>2.722222</td>\n      <td>1.333333</td>\n      <td>2.733333</td>\n      <td>2.066667</td>\n      <td>2.133333</td>\n      <td>2.100000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.555556</td>\n      <td>3.148148</td>\n      <td>1.666667</td>\n      <td>3.222222</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>2.666667</td>\n      <td>3.222222</td>\n      <td>1.333333</td>\n      <td>3.111111</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.200000</td>\n      <td>2.466667</td>\n      <td>2.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.266667</td>\n      <td>2.851852</td>\n      <td>1.388889</td>\n      <td>3.666667</td>\n      <td>1.166667</td>\n      <td>2.666667</td>\n      <td>1.333333</td>\n      <td>2.222222</td>\n      <td>1.666667</td>\n      <td>2.444444</td>\n      <td>1.500000</td>\n      <td>2.733333</td>\n      <td>2.066667</td>\n      <td>2.000000</td>\n      <td>2.033333</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2.088889</td>\n      <td>2.555556</td>\n      <td>1.388889</td>\n      <td>3.111111</td>\n      <td>1.500000</td>\n      <td>2.111111</td>\n      <td>1.333333</td>\n      <td>2.444444</td>\n      <td>1.333333</td>\n      <td>2.277778</td>\n      <td>1.333333</td>\n      <td>2.400000</td>\n      <td>1.866667</td>\n      <td>2.000000</td>\n      <td>1.933333</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2.409091</td>\n      <td>3.076923</td>\n      <td>1.444444</td>\n      <td>3.666667</td>\n      <td>1.166667</td>\n      <td>2.750000</td>\n      <td>1.333333</td>\n      <td>2.777778</td>\n      <td>1.833333</td>\n      <td>2.763889</td>\n      <td>1.583333</td>\n      <td>2.733333</td>\n      <td>2.071429</td>\n      <td>2.400000</td>\n      <td>2.235714</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2.613636</td>\n      <td>3.230769</td>\n      <td>1.722222</td>\n      <td>3.625000</td>\n      <td>1.000000</td>\n      <td>2.888889</td>\n      <td>1.833333</td>\n      <td>3.222222</td>\n      <td>2.333333</td>\n      <td>3.055556</td>\n      <td>2.083333</td>\n      <td>2.857143</td>\n      <td>2.133333</td>\n      <td>2.866667</td>\n      <td>2.500000</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>2.590909</td>\n      <td>3.222222</td>\n      <td>1.588235</td>\n      <td>3.777778</td>\n      <td>1.333333</td>\n      <td>3.000000</td>\n      <td>1.666667</td>\n      <td>2.888889</td>\n      <td>1.800000</td>\n      <td>2.944444</td>\n      <td>1.733333</td>\n      <td>2.933333</td>\n      <td>2.333333</td>\n      <td>2.500000</td>\n      <td>2.416667</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>1.933333</td>\n      <td>2.444444</td>\n      <td>1.166667</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>2.444444</td>\n      <td>1.000000</td>\n      <td>1.888889</td>\n      <td>1.500000</td>\n      <td>2.166667</td>\n      <td>1.250000</td>\n      <td>2.200000</td>\n      <td>1.866667</td>\n      <td>1.733333</td>\n      <td>1.800000</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate the average predictions for different conditions and emotions\n",
    "average_prediction = df_rating_0.mean(axis=1)\n",
    "\n",
    "average_prediction_match = df_fulfilled_1.mean(axis=1)\n",
    "average_prediction_no_match = df_fulfilled_0.mean(axis=1)\n",
    "\n",
    "average_prediction_happy_match = df_fulfilled_happy.mean(axis=1)\n",
    "average_prediction_happy_no_match = df_no_fulfilled_happy.mean(axis=1)\n",
    "\n",
    "average_prediction_sad_match = df_fulfilled_sad.mean(axis=1)\n",
    "average_prediction_sad_no_match = df_no_fulfilled_sad.mean(axis=1)\n",
    "\n",
    "average_prediction_fear_match = df_fulfilled_fear.mean(axis=1)\n",
    "average_prediction_fear_no_match = df_no_fulfilled_fear.mean(axis=1)\n",
    "\n",
    "average_prediction_happy = df_rating_happy.mean(axis=1)\n",
    "average_prediction_sad = df_rating_sad.mean(axis=1)\n",
    "average_prediction_fear = df_rating_fear.mean(axis=1)\n",
    "\n",
    "# Create a DataFrame to store the average predictions for different conditions and emotions\n",
    "average_df = pd.DataFrame([\n",
    "    average_prediction,\n",
    "    average_prediction_match,\n",
    "    average_prediction_no_match,\n",
    "    average_prediction_happy_match,\n",
    "    average_prediction_happy_no_match,\n",
    "    average_prediction_sad_match,\n",
    "    average_prediction_sad_no_match,\n",
    "    average_prediction_fear_match,\n",
    "    average_prediction_fear_no_match,\n",
    "    (average_prediction_sad_match + average_prediction_fear_match) / 2,  # Average prediction for sad and fear match\n",
    "    (average_prediction_fear_no_match + average_prediction_sad_no_match) / 2,  # Average prediction for sad and fear no match\n",
    "    average_prediction_happy,\n",
    "    average_prediction_sad,\n",
    "    average_prediction_fear,\n",
    "    (average_prediction_sad + average_prediction_fear) / 2  # Average prediction for sad and fear\n",
    "]).transpose()\n",
    "\n",
    "# Assign column names to the average prediction DataFrame\n",
    "average_df.columns = [\n",
    "    \"Mean_Rating0\",\n",
    "    \"Mean_Rating0_Match\",\n",
    "    \"Mean_Rating0_No_Match\",\n",
    "    \"Mean_Rating0_Match_Happy\",\n",
    "    \"Mean_Rating0_No_Match_Happy\",\n",
    "    \"Mean_Rating0_Match_Sad\",\n",
    "    \"Mean_Rating0_No_Match_Sad\",\n",
    "    \"Mean_Rating0_Match_Fear\",\n",
    "    \"Mean_Rating0_No_Match_Fear\",\n",
    "    \"Mean_Rating0_Match_Negative\",  # Average prediction for sad and fear match\n",
    "    \"Mean_Rating0_No_Match_Negative\",  # Average prediction for sad and fear no match\n",
    "    \"Mean_Rating0_Happy\",\n",
    "    \"Mean_Rating0_Sad\",\n",
    "    \"Mean_Rating0_Fear\",\n",
    "    \"Mean_Rating0_Negative\"  # Average prediction for sad and fear\n",
    "]\n",
    "\n",
    "# Display the average prediction DataFrame\n",
    "average_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:19.312694200Z",
     "start_time": "2024-07-14T10:26:19.231356200Z"
    }
   },
   "id": "972ae72946d19541",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Difference Match No Match\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47894354018bf91d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Dif_Match  Dif_Happy   Dif_Sad  Dif_Fear  Dif_Negative\n0     1.666667   2.388889  1.388889  1.222222      1.305556\n1     1.166667   2.000000  0.722222  0.777778      0.750000\n2     1.722222   2.388889  1.166667  1.611111      1.388889\n3     1.481481   2.222222  0.333333  1.888889      1.111111\n4     1.462963   2.500000  1.333333  0.555556      0.944444\n..         ...        ...       ...       ...           ...\n145   1.166667   1.611111  0.777778  1.111111      0.944444\n146   1.632479   2.500000  1.416667  0.944444      1.180556\n147   1.508547   2.625000  1.055556  0.888889      0.972222\n148   1.633987   2.444444  1.333333  1.088889      1.211111\n149   1.277778   2.000000  1.444444  0.388889      0.916667\n\n[150 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dif_Match</th>\n      <th>Dif_Happy</th>\n      <th>Dif_Sad</th>\n      <th>Dif_Fear</th>\n      <th>Dif_Negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.666667</td>\n      <td>2.388889</td>\n      <td>1.388889</td>\n      <td>1.222222</td>\n      <td>1.305556</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.166667</td>\n      <td>2.000000</td>\n      <td>0.722222</td>\n      <td>0.777778</td>\n      <td>0.750000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.722222</td>\n      <td>2.388889</td>\n      <td>1.166667</td>\n      <td>1.611111</td>\n      <td>1.388889</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.481481</td>\n      <td>2.222222</td>\n      <td>0.333333</td>\n      <td>1.888889</td>\n      <td>1.111111</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.462963</td>\n      <td>2.500000</td>\n      <td>1.333333</td>\n      <td>0.555556</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>1.166667</td>\n      <td>1.611111</td>\n      <td>0.777778</td>\n      <td>1.111111</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>1.632479</td>\n      <td>2.500000</td>\n      <td>1.416667</td>\n      <td>0.944444</td>\n      <td>1.180556</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>1.508547</td>\n      <td>2.625000</td>\n      <td>1.055556</td>\n      <td>0.888889</td>\n      <td>0.972222</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1.633987</td>\n      <td>2.444444</td>\n      <td>1.333333</td>\n      <td>1.088889</td>\n      <td>1.211111</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>1.277778</td>\n      <td>2.000000</td>\n      <td>1.444444</td>\n      <td>0.388889</td>\n      <td>0.916667</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the differences between matched and non-matched average predictions\n",
    "dif_match = average_prediction_match - average_prediction_no_match\n",
    "\n",
    "# Calculate the differences between matched and non-matched average predictions for each emotion\n",
    "dif_happy = average_prediction_happy_match - average_prediction_happy_no_match\n",
    "dif_sad = average_prediction_sad_match - average_prediction_sad_no_match\n",
    "dif_fear = average_prediction_fear_match - average_prediction_fear_no_match\n",
    "\n",
    "# Create a DataFrame to store the differences\n",
    "dif_df = pd.DataFrame([\n",
    "    dif_match,\n",
    "    dif_happy,\n",
    "    dif_sad,\n",
    "    dif_fear,\n",
    "    (dif_sad + dif_fear) / 2  # Average difference for negative emotions (sad and fear)\n",
    "]).transpose()\n",
    "\n",
    "# Assign column names to the differences DataFrame\n",
    "dif_df.columns = [\n",
    "    \"Dif_Match\",\n",
    "    \"Dif_Happy\",\n",
    "    \"Dif_Sad\",\n",
    "    \"Dif_Fear\",\n",
    "    \"Dif_Negative\"  # Average difference for negative emotions (sad and fear)\n",
    "]\n",
    "\n",
    "# Display the differences DataFrame\n",
    "dif_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:19.351475900Z",
     "start_time": "2024-07-14T10:26:19.278985600Z"
    }
   },
   "id": "52f74cacc982ff0a",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "theory_df = pd.concat([average_df,dif_df,correlation_df],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:19.353526100Z",
     "start_time": "2024-07-14T10:26:19.306688Z"
    }
   },
   "id": "f27321e1cc2ee7d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining Both Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b2048cdd51ad4ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([theory_df,time_df.drop(\"Subject\",axis=1)],axis=1)\n",
    "full_dataset.insert(0,\"Subject\",RETOS_BEBRASK_dataset[\"DataFile.Basename\"])\n",
    "full_dataset.to_excel('../Clustering_Predictive_Processing/All_Features_dataset.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-14T10:26:19.552498500Z",
     "start_time": "2024-07-14T10:26:19.322186Z"
    }
   },
   "id": "17b9fa728443acae",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e62d3fb1676980"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
