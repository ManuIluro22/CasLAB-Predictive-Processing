{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:02.795983500Z",
     "start_time": "2024-05-08T10:52:02.773788600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RETOS_BEBRASK_dataset = pd.read_excel(\"RETOS_BEBRASK_long.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:03.650062600Z",
     "start_time": "2024-05-08T10:52:03.006411400Z"
    }
   },
   "id": "85bc66ee2813d5c3",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def slope_intercept_regression(time_series):\n",
    "    slopes = np.zeros(time_series.shape[0])\n",
    "    intercepts = np.zeros(time_series.shape[0])\n",
    "    \n",
    "    # Original time stamps as the independent variable\n",
    "    X_original = np.arange(1, time_series.shape[1] + 1)\n",
    "    \n",
    "    # Loop over each subject to fit a linear model and extract the slope\n",
    "    for i in range(time_series.shape[0]):\n",
    "        # Time series values for the current subject, dropping NaNs\n",
    "        Y = time_series.iloc[i, :].dropna().values\n",
    "    \n",
    "        # Ensure Y is of numeric type\n",
    "        Y = pd.to_numeric(Y, errors='coerce').astype('float64')\n",
    "    \n",
    "        # Filter X based on the non-NaN entries of Y to maintain correspondence\n",
    "        X_filtered = X_original[~time_series.iloc[i, :].isna()]\n",
    "    \n",
    "        # Ensure X_filtered is of numeric type\n",
    "        X_filtered = pd.to_numeric(X_filtered, errors='coerce').astype('float64')\n",
    "    \n",
    "        # Check again after conversion to avoid fitting a model with insufficient data\n",
    "        if len(Y) > 1 and not np.isnan(Y).all():\n",
    "            slope, intercept = np.polyfit(X_filtered, Y, 1)  # Fit a linear model\n",
    "            slopes[i] = slope  # Store the slope\n",
    "            intercepts[i] = intercept\n",
    "        else:\n",
    "            slopes[i] = np.nan  # Assign NaN if not enough data points or if conversion resulted in NaNs\n",
    "            intercepts[i] = np.nan\n",
    "    return slopes,intercepts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:03.687196900Z",
     "start_time": "2024-05-08T10:52:03.659269100Z"
    }
   },
   "id": "73bfded615d3c000",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Time series Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f6cf37c2110ecc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting info on predictability"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "688fa4fd7c069ec1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "subject_id = RETOS_BEBRASK_dataset.iloc[:, 0]\n",
    "subject_timeseries = RETOS_BEBRASK_dataset.iloc[:, 1:46]\n",
    "\n",
    "rating_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Rating0' in col]\n",
    "fulfilled_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Fulfilled' in col]\n",
    "emotions_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'EvokedEmotion' in col]\n",
    "\n",
    "subjects = RETOS_BEBRASK_dataset.index\n",
    "\n",
    "df_rating_0 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(45)])\n",
    "\n",
    "df_fulfilled_1 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(30)])\n",
    "df_fulfilled_0 = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(20)])\n",
    "\n",
    "# Initialize the DataFrames for each emotion with 'Fulfilled' status\n",
    "df_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "\n",
    "df_no_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_no_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_no_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "\n",
    "# Iterate through each subject\n",
    "for subject in subjects:\n",
    "    ratings = []\n",
    "    # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0\n",
    "    ratings_1,ratings_0 = [], []\n",
    "        # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0 for each emotion\n",
    "    ratings_1_happy, ratings_1_sad, ratings_1_fear = [], [], []\n",
    "    ratings_0_happy, ratings_0_sad, ratings_0_fear = [], [], []\n",
    "\n",
    "\n",
    "    # Iterate through each 'Fulfilled' and 'Rating0' column pair\n",
    "    for fulfilled_col, rating_col, emotion_col in zip(fulfilled_columns, rating_columns, emotions_columns):\n",
    "        fulfilled_status = RETOS_BEBRASK_dataset.loc[subject, fulfilled_col]\n",
    "        rating_value = RETOS_BEBRASK_dataset.loc[subject, rating_col]\n",
    "        emotion_value = RETOS_BEBRASK_dataset.loc[subject, emotion_col]\n",
    "        ratings.append(rating_value)\n",
    "        if fulfilled_status == 1:\n",
    "            ratings_1.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                ratings_1_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                ratings_1_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                ratings_1_fear.append(rating_value)\n",
    "        elif fulfilled_status == 0:\n",
    "            ratings_0.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                ratings_0_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                ratings_0_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                ratings_0_fear.append(rating_value)\n",
    "    \n",
    "    # Extend lists with NaN values if they are shorter than required lengths\n",
    "    \n",
    "    ratings_1.extend([np.nan] * (30 - len(ratings_1)))\n",
    "    ratings_0.extend([np.nan] * (20 - len(ratings_0)))\n",
    "\n",
    "    ratings_1_happy.extend([np.nan] * (11 - len(ratings_1_happy)))\n",
    "    ratings_1_sad.extend([np.nan] * (11 - len(ratings_1_sad)))\n",
    "    ratings_1_fear.extend([np.nan] * (11 - len(ratings_1_fear)))\n",
    "\n",
    "    ratings_0_happy.extend([np.nan] * (7 - len(ratings_0_happy)))\n",
    "    ratings_0_sad.extend([np.nan] * (7 - len(ratings_0_sad)))\n",
    "    ratings_0_fear.extend([np.nan] * (7 - len(ratings_0_fear)))\n",
    "\n",
    "    # Insert lists into their respective DataFrames for the current subject\n",
    "    df_rating_0.loc[subject, :] = ratings\n",
    "    \n",
    "    df_fulfilled_1.loc[subject, :] = ratings_1\n",
    "    df_fulfilled_0.loc[subject, :] = ratings_0\n",
    "\n",
    "    df_fulfilled_happy.loc[subject, :] = ratings_1_happy\n",
    "    df_fulfilled_sad.loc[subject, :] = ratings_1_sad\n",
    "    df_fulfilled_fear.loc[subject, :] = ratings_1_fear\n",
    "\n",
    "    df_no_fulfilled_happy.loc[subject, :] = ratings_0_happy\n",
    "    df_no_fulfilled_sad.loc[subject, :] = ratings_0_sad\n",
    "    df_no_fulfilled_fear.loc[subject, :] = ratings_0_fear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:04.137957Z",
     "start_time": "2024-05-08T10:52:03.666553200Z"
    }
   },
   "id": "f1c8ff7dc1afd11c",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "slopes_match, intercepts_match= slope_intercept_regression(df_fulfilled_1)\n",
    "slopes_no_match, intercepts_no_match= slope_intercept_regression(df_fulfilled_0)\n",
    "\n",
    "slopes_match_happy, intercepts_match_happy = slope_intercept_regression(df_fulfilled_happy)\n",
    "slopes_match_no_happy, intercepts_match_no_happy = slope_intercept_regression(df_no_fulfilled_happy)\n",
    "\n",
    "slopes_match_sad, intercepts_match_sad = slope_intercept_regression(df_fulfilled_sad)\n",
    "slopes_match_no_sad, intercepts_match_no_sad = slope_intercept_regression(df_no_fulfilled_sad)\n",
    "\n",
    "slopes_match_fear, intercepts_match_fear = slope_intercept_regression(df_fulfilled_fear)\n",
    "slopes_match_no_fear, intercepts_match_no_fear = slope_intercept_regression(df_no_fulfilled_fear)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:04.812306800Z",
     "start_time": "2024-05-08T10:52:04.170960900Z"
    }
   },
   "id": "93ee8f315edde439",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame([slopes_match,slopes_no_match,intercepts_match,intercepts_no_match,slopes_match_happy,slopes_match_no_happy,intercepts_match_happy,intercepts_match_no_happy,slopes_match_sad,slopes_match_no_sad,intercepts_match_sad,intercepts_match_no_sad,\n",
    "                               slopes_match_fear,slopes_match_no_fear,intercepts_match_fear,intercepts_match_no_fear]).transpose()\n",
    "\n",
    "time_df.columns = [\"Trend_Match\",\"Trend_No_Match\",\"Intercept_Match\",\"Intercept_No_Match\",\"Trend_Match_Happy\",\"Trend_No_Match_Happy\",\"Intercept_Match_Happy\",\"Intercept_No_Match_Happy\",\"Trend_Match_Sad\",\"Trend_No_Match_Sad\",\"Intercept_Match_Sad\",\"Intercept_No_Match_Sad\",\n",
    "                   \"Trend_Match_Fear\",\"Trend_No_Match_Fear\",\"Intercept_Match_Fear\",\"Intercept_No_Match_Fear\"]\n",
    "\n",
    "time_df[\"Final_Value_Match\"] = time_df[\"Intercept_Match\"] + time_df[\"Trend_Match\"]*27\n",
    "time_df[\"Final_Value_No_Match\"] = time_df[\"Intercept_No_Match\"] + time_df[\"Trend_No_Match\"]*18\n",
    "\n",
    "time_df[\"Final_Value_Match_Happy\"] = time_df[\"Intercept_Match_Happy\"] + time_df[\n",
    "    \"Trend_Match_Happy\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Happy\"] = time_df[\"Intercept_No_Match_Happy\"] + time_df[\n",
    "    \"Trend_No_Match_Happy\"] * 6\n",
    "\n",
    "time_df[\"Final_Value_Match_Sad\"] = time_df[\"Intercept_Match_Sad\"] + time_df[\"Trend_Match_Sad\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Sad\"] = time_df[\"Intercept_No_Match_Sad\"] + time_df[\n",
    "    \"Trend_No_Match_Sad\"] * 6\n",
    "\n",
    "time_df[\"Final_Value_Match_Fear\"] = time_df[\"Intercept_Match_Fear\"] + time_df[\n",
    "    \"Trend_Match_Fear\"] * 9\n",
    "time_df[\"Final_Value_No_Match_Fear\"] = time_df[\"Intercept_No_Match_Fear\"] + time_df[\n",
    "    \"Trend_No_Match_Fear\"] * 6\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:04.836524Z",
     "start_time": "2024-05-08T10:52:04.795296500Z"
    }
   },
   "id": "41702b29462cc457",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theory Driven Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92091222ec5bf4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting info on likeability"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcff2dd04672f7ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fulfilled_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Fulfilled' in col]\n",
    "emotions_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'EvokedEmotion' in col]\n",
    "likeability_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Rating.' in col]\n",
    "\n",
    "subjects = RETOS_BEBRASK_dataset.index\n",
    "\n",
    "\n",
    "df_likeability = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(45)])\n",
    "\n",
    "df_likeability_fulfilled = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(30)])\n",
    "df_likeability_no_fulfilled = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(20)])\n",
    "\n",
    "# Initialize the DataFrames for each emotion with 'Fulfilled' status\n",
    "df_likeability_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_likeability_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "df_likeability_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(11)])\n",
    "\n",
    "df_likeability_no_fulfilled_happy = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_likeability_no_fulfilled_sad = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "df_likeability_no_fulfilled_fear = pd.DataFrame(index=subjects, columns=[f\"Time_{i+1}\" for i in range(7)])\n",
    "\n",
    "# Iterate through each subject\n",
    "for subject in subjects:\n",
    "    likeability = []\n",
    "    # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0\n",
    "    likeability_1,likeability_0 = [], []\n",
    "        # Initialize empty lists for 'Fulfilled' = 1 and 'Fulfilled' = 0 for each emotion\n",
    "    likeability_1_happy, likeability_1_sad, likeability_1_fear = [], [], []\n",
    "    likeability_0_happy, likeability_0_sad, likeability_0_fear = [], [], []\n",
    "\n",
    "\n",
    "    # Iterate through each 'Fulfilled' and 'Rating0' column pair\n",
    "    for fulfilled_col, likeability_col, emotion_col in zip(fulfilled_columns, likeability_columns, emotions_columns):\n",
    "        fulfilled_status = RETOS_BEBRASK_dataset.loc[subject, fulfilled_col]\n",
    "        rating_value = RETOS_BEBRASK_dataset.loc[subject, likeability_col]\n",
    "        emotion_value = RETOS_BEBRASK_dataset.loc[subject, emotion_col]\n",
    "        likeability.append(rating_value)\n",
    "        if fulfilled_status == 1:\n",
    "            likeability_1.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                likeability_1_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                likeability_1_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                likeability_1_fear.append(rating_value)\n",
    "        elif fulfilled_status == 0:\n",
    "            likeability_0.append(rating_value)\n",
    "\n",
    "            if emotion_value == \"happiness\":\n",
    "                likeability_0_happy.append(rating_value)\n",
    "            elif emotion_value == \"sadness\":\n",
    "                likeability_0_sad.append(rating_value)\n",
    "            elif emotion_value == \"fear\":\n",
    "                likeability_0_fear.append(rating_value)\n",
    "    \n",
    "    # Extend lists with NaN values if they are shorter than required lengths\n",
    "    \n",
    "    likeability_1.extend([np.nan] * (30 - len(likeability_1)))\n",
    "    likeability_0.extend([np.nan] * (20 - len(likeability_0)))\n",
    "\n",
    "    likeability_1_happy.extend([np.nan] * (11 - len(likeability_1_happy)))\n",
    "    likeability_1_sad.extend([np.nan] * (11 - len(likeability_1_sad)))\n",
    "    likeability_1_fear.extend([np.nan] * (11 - len(likeability_1_fear)))\n",
    "\n",
    "    likeability_0_happy.extend([np.nan] * (7 - len(likeability_0_happy)))\n",
    "    likeability_0_sad.extend([np.nan] * (7 - len(likeability_0_sad)))\n",
    "    likeability_0_fear.extend([np.nan] * (7 - len(likeability_0_fear)))\n",
    "\n",
    "    # Insert lists into their respective DataFrames for the current subject\n",
    "    df_likeability.loc[subject, :] = likeability\n",
    "    \n",
    "    df_likeability_fulfilled.loc[subject, :] = likeability_1\n",
    "    df_likeability_no_fulfilled.loc[subject, :] = likeability_0\n",
    "\n",
    "    df_likeability_fulfilled_happy.loc[subject, :] = likeability_1_happy\n",
    "    df_likeability_fulfilled_sad.loc[subject, :] = likeability_1_sad\n",
    "    df_likeability_fulfilled_fear.loc[subject, :] = likeability_1_fear\n",
    "\n",
    "    df_likeability_no_fulfilled_happy.loc[subject, :] = likeability_0_happy\n",
    "    df_likeability_no_fulfilled_sad.loc[subject, :] = likeability_0_sad\n",
    "    df_likeability_no_fulfilled_fear.loc[subject, :] = likeability_0_fear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:05.261796400Z",
     "start_time": "2024-05-08T10:52:04.893137700Z"
    }
   },
   "id": "3381c86b63b39b6",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Correlations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9919eec9bb06f877"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Cor_Pred_Like  Cor_Pred_Like_Match  Cor_Pred_Like_No_Match  \\\n0         0.113495             0.209359                0.103807   \n1         0.214781             0.400320               -0.070711   \n2         0.250345             0.432551                0.139859   \n3        -0.230171            -0.006600               -0.585369   \n4         0.176201             0.334275               -0.194348   \n..             ...                  ...                     ...   \n145       0.255697             0.281127                0.260715   \n146       0.608424             0.418441                0.346488   \n147       0.440557             0.508058                0.000000   \n148       0.376099             0.486755               -0.258969   \n149       0.625892             0.522540                0.582886   \n\n     Cor_Pred_Like_Match_Happy  Cor_Pred_Like_No_Match_Happy  \\\n0                -2.500000e-01                      0.542326   \n1                 4.902903e-01                     -0.632456   \n2                -2.294157e-01                      0.800000   \n3                 1.409710e-02                      0.000000   \n4                 5.345225e-01                      0.415227   \n..                         ...                           ...   \n145               9.433333e-02                      0.727607   \n146               2.467162e-17                     -0.108465   \n147               7.581298e-01                      0.000000   \n148               1.000000e+00                      0.612372   \n149               5.883484e-01                      0.000000   \n\n     Cor_Pred_Like_Match_Sad  Cor_Pred_Like_No_Match_Sad  \\\n0                   0.342381                   -0.061430   \n1                   0.059761                    0.000000   \n2                   0.230940                    0.000000   \n3                   0.000000                   -0.542326   \n4                   0.162221                   -0.387298   \n..                       ...                         ...   \n145                -0.036564                    0.342997   \n146                -0.125000                    0.542326   \n147                 0.346423                   -0.121566   \n148                 0.000000                   -0.800000   \n149                 0.283473                    0.000000   \n\n     Cor_Pred_Like_Match_Fear  Cor_Pred_Like_No_Match_Fear  \n0                    0.196722                    -0.188982  \n1                    0.395285                     0.158114  \n2                    0.307692                    -0.307148  \n3                   -0.280000                    -0.131306  \n4                   -0.762493                    -0.867722  \n..                        ...                          ...  \n145                  0.463586                    -0.316228  \n146                  0.304997                     0.242536  \n147                  0.548795                    -0.242536  \n148                  0.202031                    -0.542326  \n149                  0.363774                     0.801784  \n\n[150 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cor_Pred_Like</th>\n      <th>Cor_Pred_Like_Match</th>\n      <th>Cor_Pred_Like_No_Match</th>\n      <th>Cor_Pred_Like_Match_Happy</th>\n      <th>Cor_Pred_Like_No_Match_Happy</th>\n      <th>Cor_Pred_Like_Match_Sad</th>\n      <th>Cor_Pred_Like_No_Match_Sad</th>\n      <th>Cor_Pred_Like_Match_Fear</th>\n      <th>Cor_Pred_Like_No_Match_Fear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.113495</td>\n      <td>0.209359</td>\n      <td>0.103807</td>\n      <td>-2.500000e-01</td>\n      <td>0.542326</td>\n      <td>0.342381</td>\n      <td>-0.061430</td>\n      <td>0.196722</td>\n      <td>-0.188982</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.214781</td>\n      <td>0.400320</td>\n      <td>-0.070711</td>\n      <td>4.902903e-01</td>\n      <td>-0.632456</td>\n      <td>0.059761</td>\n      <td>0.000000</td>\n      <td>0.395285</td>\n      <td>0.158114</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.250345</td>\n      <td>0.432551</td>\n      <td>0.139859</td>\n      <td>-2.294157e-01</td>\n      <td>0.800000</td>\n      <td>0.230940</td>\n      <td>0.000000</td>\n      <td>0.307692</td>\n      <td>-0.307148</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.230171</td>\n      <td>-0.006600</td>\n      <td>-0.585369</td>\n      <td>1.409710e-02</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.542326</td>\n      <td>-0.280000</td>\n      <td>-0.131306</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.176201</td>\n      <td>0.334275</td>\n      <td>-0.194348</td>\n      <td>5.345225e-01</td>\n      <td>0.415227</td>\n      <td>0.162221</td>\n      <td>-0.387298</td>\n      <td>-0.762493</td>\n      <td>-0.867722</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0.255697</td>\n      <td>0.281127</td>\n      <td>0.260715</td>\n      <td>9.433333e-02</td>\n      <td>0.727607</td>\n      <td>-0.036564</td>\n      <td>0.342997</td>\n      <td>0.463586</td>\n      <td>-0.316228</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>0.608424</td>\n      <td>0.418441</td>\n      <td>0.346488</td>\n      <td>2.467162e-17</td>\n      <td>-0.108465</td>\n      <td>-0.125000</td>\n      <td>0.542326</td>\n      <td>0.304997</td>\n      <td>0.242536</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0.440557</td>\n      <td>0.508058</td>\n      <td>0.000000</td>\n      <td>7.581298e-01</td>\n      <td>0.000000</td>\n      <td>0.346423</td>\n      <td>-0.121566</td>\n      <td>0.548795</td>\n      <td>-0.242536</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.376099</td>\n      <td>0.486755</td>\n      <td>-0.258969</td>\n      <td>1.000000e+00</td>\n      <td>0.612372</td>\n      <td>0.000000</td>\n      <td>-0.800000</td>\n      <td>0.202031</td>\n      <td>-0.542326</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0.625892</td>\n      <td>0.522540</td>\n      <td>0.582886</td>\n      <td>5.883484e-01</td>\n      <td>0.000000</td>\n      <td>0.283473</td>\n      <td>0.000000</td>\n      <td>0.363774</td>\n      <td>0.801784</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_correlation(row1, row2):\n",
    "    # Check if the standard deviation is zero\n",
    "    if row1.std() == 0 or row2.std() == 0:\n",
    "        # Handle the constant data case here, e.g., return 1 or None\n",
    "        return 1 if row1.equals(row2) else 0\n",
    "    return row1.corr(row2)\n",
    "# Apply the function row-wise using the DataFrame `apply` method\n",
    "correlations_all = [row_correlation(df_likeability.loc[index], df_rating_0.loc[index]) for index in df_likeability.index]\n",
    "correlations_match = [row_correlation(df_likeability_fulfilled.loc[index], df_fulfilled_1.loc[index]) for index in df_likeability_fulfilled.index]\n",
    "correlations_no_match = [row_correlation(df_likeability_no_fulfilled.loc[index], df_fulfilled_0.loc[index]) for index in df_likeability_no_fulfilled.index]\n",
    "correlations_happy_match = [row_correlation(df_likeability_fulfilled_happy.loc[index], df_fulfilled_happy.loc[index]) for index in df_likeability_fulfilled_happy.index]\n",
    "correlations_happy_no_match = [row_correlation(df_likeability_no_fulfilled_happy.loc[index], df_no_fulfilled_happy.loc[index]) for index in df_likeability_no_fulfilled_happy.index]\n",
    "correlations_sad_match = [row_correlation(df_likeability_fulfilled_sad.loc[index], df_fulfilled_sad.loc[index]) for index in df_likeability_fulfilled_sad.index]\n",
    "correlations_sad_no_match = [row_correlation(df_likeability_no_fulfilled_sad.loc[index], df_no_fulfilled_sad.loc[index]) for index in df_likeability_no_fulfilled_sad.index]\n",
    "correlations_fear_match = [row_correlation(df_likeability_fulfilled_fear.loc[index], df_fulfilled_fear.loc[index]) for index in df_likeability_fulfilled_fear.index]\n",
    "correlations_fear_no_match = [row_correlation(df_likeability_no_fulfilled_fear.loc[index], df_no_fulfilled_fear.loc[index]) for index in df_likeability_no_fulfilled_fear.index]\n",
    "\n",
    "\n",
    "correlation_df = pd.DataFrame([correlations_all,correlations_match,correlations_no_match,correlations_happy_match,correlations_happy_no_match,correlations_sad_match,correlations_sad_no_match,correlations_fear_match,correlations_fear_no_match]).transpose()\n",
    "correlation_df.columns = [\"Cor_Pred_Like\",\"Cor_Pred_Like_Match\",\"Cor_Pred_Like_No_Match\",\"Cor_Pred_Like_Match_Happy\",\"Cor_Pred_Like_No_Match_Happy\",\"Cor_Pred_Like_Match_Sad\",\"Cor_Pred_Like_No_Match_Sad\",\"Cor_Pred_Like_Match_Fear\",\"Cor_Pred_Like_No_Match_Fear\"]\n",
    "correlation_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T10:52:06.331918300Z",
     "start_time": "2024-05-08T10:52:05.277310800Z"
    }
   },
   "id": "177d4dd5d9980875",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Average Rating"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21514fae7c976b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Mean_Rating0  Mean_Rating0_Match  Mean_Rating0_No_Match  \\\n0        2.888889            3.555556               1.888889   \n1        2.533333            3.000000               1.833333   \n2        2.311111            3.000000               1.277778   \n3        2.555556            3.148148               1.666667   \n4        2.266667            2.851852               1.388889   \n..            ...                 ...                    ...   \n145      2.088889            2.555556               1.388889   \n146      2.422222            3.074074               1.444444   \n147      2.622222            3.222222               1.722222   \n148      2.577778            3.222222               1.611111   \n149      1.933333            2.444444               1.166667   \n\n     Mean_Rating0_Match_Happy  Mean_Rating0_No_Match_Happy  \\\n0                    3.888889                     1.500000   \n1                    3.333333                     1.333333   \n2                    3.555556                     1.166667   \n3                    3.222222                     1.000000   \n4                    3.666667                     1.166667   \n..                        ...                          ...   \n145                  3.111111                     1.500000   \n146                  3.666667                     1.166667   \n147                  3.555556                     1.000000   \n148                  3.777778                     1.333333   \n149                  3.000000                     1.000000   \n\n     Mean_Rating0_Match_Sad  Mean_Rating0_No_Match_Sad  \\\n0                  3.555556                   2.166667   \n1                  2.555556                   1.833333   \n2                  2.666667                   1.500000   \n3                  3.000000                   2.666667   \n4                  2.666667                   1.333333   \n..                      ...                        ...   \n145                2.111111                   1.333333   \n146                2.777778                   1.333333   \n147                2.888889                   1.833333   \n148                3.000000                   1.666667   \n149                2.444444                   1.000000   \n\n     Mean_Rating0_Match_Fear  Mean_Rating0_No_Match_Fear  \n0                   3.222222                    2.000000  \n1                   3.111111                    2.333333  \n2                   2.777778                    1.166667  \n3                   3.222222                    1.333333  \n4                   2.222222                    1.666667  \n..                       ...                         ...  \n145                 2.444444                    1.333333  \n146                 2.777778                    1.833333  \n147                 3.222222                    2.333333  \n148                 2.888889                    1.833333  \n149                 1.888889                    1.500000  \n\n[150 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean_Rating0</th>\n      <th>Mean_Rating0_Match</th>\n      <th>Mean_Rating0_No_Match</th>\n      <th>Mean_Rating0_Match_Happy</th>\n      <th>Mean_Rating0_No_Match_Happy</th>\n      <th>Mean_Rating0_Match_Sad</th>\n      <th>Mean_Rating0_No_Match_Sad</th>\n      <th>Mean_Rating0_Match_Fear</th>\n      <th>Mean_Rating0_No_Match_Fear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.888889</td>\n      <td>3.555556</td>\n      <td>1.888889</td>\n      <td>3.888889</td>\n      <td>1.500000</td>\n      <td>3.555556</td>\n      <td>2.166667</td>\n      <td>3.222222</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.533333</td>\n      <td>3.000000</td>\n      <td>1.833333</td>\n      <td>3.333333</td>\n      <td>1.333333</td>\n      <td>2.555556</td>\n      <td>1.833333</td>\n      <td>3.111111</td>\n      <td>2.333333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.311111</td>\n      <td>3.000000</td>\n      <td>1.277778</td>\n      <td>3.555556</td>\n      <td>1.166667</td>\n      <td>2.666667</td>\n      <td>1.500000</td>\n      <td>2.777778</td>\n      <td>1.166667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.555556</td>\n      <td>3.148148</td>\n      <td>1.666667</td>\n      <td>3.222222</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>2.666667</td>\n      <td>3.222222</td>\n      <td>1.333333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.266667</td>\n      <td>2.851852</td>\n      <td>1.388889</td>\n      <td>3.666667</td>\n      <td>1.166667</td>\n      <td>2.666667</td>\n      <td>1.333333</td>\n      <td>2.222222</td>\n      <td>1.666667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2.088889</td>\n      <td>2.555556</td>\n      <td>1.388889</td>\n      <td>3.111111</td>\n      <td>1.500000</td>\n      <td>2.111111</td>\n      <td>1.333333</td>\n      <td>2.444444</td>\n      <td>1.333333</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2.422222</td>\n      <td>3.074074</td>\n      <td>1.444444</td>\n      <td>3.666667</td>\n      <td>1.166667</td>\n      <td>2.777778</td>\n      <td>1.333333</td>\n      <td>2.777778</td>\n      <td>1.833333</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2.622222</td>\n      <td>3.222222</td>\n      <td>1.722222</td>\n      <td>3.555556</td>\n      <td>1.000000</td>\n      <td>2.888889</td>\n      <td>1.833333</td>\n      <td>3.222222</td>\n      <td>2.333333</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>2.577778</td>\n      <td>3.222222</td>\n      <td>1.611111</td>\n      <td>3.777778</td>\n      <td>1.333333</td>\n      <td>3.000000</td>\n      <td>1.666667</td>\n      <td>2.888889</td>\n      <td>1.833333</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>1.933333</td>\n      <td>2.444444</td>\n      <td>1.166667</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>2.444444</td>\n      <td>1.000000</td>\n      <td>1.888889</td>\n      <td>1.500000</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_prediction = df_rating_0.mean(axis=1)\n",
    "\n",
    "average_prediction_match = df_fulfilled_1.mean(axis=1)\n",
    "average_prediction_no_match = df_fulfilled_0.mean(axis=1)\n",
    "\n",
    "average_prediction_happy_match = df_fulfilled_happy.mean(axis=1)\n",
    "average_prediction_happy_no_match = df_no_fulfilled_happy.mean(axis=1)\n",
    "\n",
    "average_prediction_sad_match = df_fulfilled_sad.mean(axis=1)\n",
    "average_prediction_sad_no_match = df_no_fulfilled_sad.mean(axis=1)\n",
    "\n",
    "average_prediction_fear_match = df_fulfilled_fear.mean(axis=1)\n",
    "average_prediction_fear_no_match = df_no_fulfilled_fear.mean(axis=1)\n",
    "\n",
    "average_df = pd.DataFrame([average_prediction,average_prediction_match,average_prediction_no_match,average_prediction_happy_match,average_prediction_happy_no_match,average_prediction_sad_match,average_prediction_sad_no_match,average_prediction_fear_match,average_prediction_fear_no_match]).transpose()\n",
    "average_df.columns = [\"Mean_Rating0\",\"Mean_Rating0_Match\",\"Mean_Rating0_No_Match\",\"Mean_Rating0_Match_Happy\",\"Mean_Rating0_No_Match_Happy\",\"Mean_Rating0_Match_Sad\",\"Mean_Rating0_No_Match_Sad\",\"Mean_Rating0_Match_Fear\",\"Mean_Rating0_No_Match_Fear\"]\n",
    "average_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:44.150884Z",
     "start_time": "2024-05-08T16:24:44.040310600Z"
    }
   },
   "id": "a9c00c8435aea24",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Difference Match No Match"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a812d7703ad4749"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dif_match = average_prediction_match - average_prediction_no_match\n",
    "\n",
    "dif_happy = average_prediction_happy_match - average_prediction_happy_no_match\n",
    "dif_sad = average_prediction_sad_match - average_prediction_sad_no_match\n",
    "dif_fear = average_prediction_fear_match - average_prediction_fear_no_match\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:45.370199400Z",
     "start_time": "2024-05-08T16:24:45.350115800Z"
    }
   },
   "id": "b2f29b0c0617ca7",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     Dif_Match  Dif_Happy   Dif_Sad  Dif_Fear\n0     1.666667   2.388889  1.388889  1.222222\n1     1.166667   2.000000  0.722222  0.777778\n2     1.722222   2.388889  1.166667  1.611111\n3     1.481481   2.222222  0.333333  1.888889\n4     1.462963   2.500000  1.333333  0.555556\n..         ...        ...       ...       ...\n145   1.166667   1.611111  0.777778  1.111111\n146   1.629630   2.500000  1.444444  0.944444\n147   1.500000   2.555556  1.055556  0.888889\n148   1.611111   2.444444  1.333333  1.055556\n149   1.277778   2.000000  1.444444  0.388889\n\n[150 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dif_Match</th>\n      <th>Dif_Happy</th>\n      <th>Dif_Sad</th>\n      <th>Dif_Fear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.666667</td>\n      <td>2.388889</td>\n      <td>1.388889</td>\n      <td>1.222222</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.166667</td>\n      <td>2.000000</td>\n      <td>0.722222</td>\n      <td>0.777778</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.722222</td>\n      <td>2.388889</td>\n      <td>1.166667</td>\n      <td>1.611111</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.481481</td>\n      <td>2.222222</td>\n      <td>0.333333</td>\n      <td>1.888889</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.462963</td>\n      <td>2.500000</td>\n      <td>1.333333</td>\n      <td>0.555556</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>1.166667</td>\n      <td>1.611111</td>\n      <td>0.777778</td>\n      <td>1.111111</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>1.629630</td>\n      <td>2.500000</td>\n      <td>1.444444</td>\n      <td>0.944444</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>1.500000</td>\n      <td>2.555556</td>\n      <td>1.055556</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1.611111</td>\n      <td>2.444444</td>\n      <td>1.333333</td>\n      <td>1.055556</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>1.277778</td>\n      <td>2.000000</td>\n      <td>1.444444</td>\n      <td>0.388889</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dif_df = pd.DataFrame(\n",
    "    [dif_match, dif_happy, dif_sad, dif_fear]).transpose()\n",
    "dif_df.columns = [\"Dif_Match\",\"Dif_Happy\",\"Dif_Sad\",\"Dif_Fear\"]\n",
    "dif_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:45.598060Z",
     "start_time": "2024-05-08T16:24:45.559805800Z"
    }
   },
   "id": "f234b6174d15cd9a",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "theory_df = pd.concat([average_df,dif_df,correlation_df],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:46.343617100Z",
     "start_time": "2024-05-08T16:24:46.313137600Z"
    }
   },
   "id": "20052604499a7be3",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining Both Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8297798b00b87b2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([theory_df,time_df],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:46.371667Z",
     "start_time": "2024-05-08T16:24:46.326224800Z"
    }
   },
   "id": "8160e52287d043a3",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0         PREDWELL_RETOS-1-1\n1        PREDWELL_RETOS-10-1\n2       PREDWELL_RETOS-101-1\n3       PREDWELL_RETOS-102-1\n4       PREDWELL_RETOS-103-1\n               ...          \n145    PREDWELL_RETOS-1124-1\n146    PREDWELL_RETOS-1125-1\n147    PREDWELL_RETOS-1126-1\n148    PREDWELL_RETOS-1127-1\n149    PREDWELL_RETOS-1128-1\nName: DataFile.Basename, Length: 150, dtype: object"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RETOS_BEBRASK_dataset[\"DataFile.Basename\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:46.606081100Z",
     "start_time": "2024-05-08T16:24:46.591554900Z"
    }
   },
   "id": "3cf12dac9d126df1",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_dataset.insert(0,\"Subject\",RETOS_BEBRASK_dataset[\"DataFile.Basename\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:46.900302500Z",
     "start_time": "2024-05-08T16:24:46.858112500Z"
    }
   },
   "id": "ddb67fb7028c443a",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_dataset.to_excel('../Clustering_Predictive_Processing/All_Features_dataset.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T16:24:47.548920100Z",
     "start_time": "2024-05-08T16:24:47.331148200Z"
    }
   },
   "id": "be3b6aead8587e84",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e32d33881b3415"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
